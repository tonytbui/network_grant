{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import statistics as stats\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev = {'directed_forgetting_condition':'DF',\n",
    "#           'directed_condition': 'DF',\n",
    "         'flanker_condition':'FLANKER',\n",
    "         'go_nogo_condition':'GNG',\n",
    "         'n_back_condition':'NBACK',\n",
    "          'delay_condition':'DELAY',\n",
    "         'predictable_condition':'PREDICT',\n",
    "#           'predictive_condition':'PREDICT',\n",
    "         'shape_matching_condition':'SHAPE',\n",
    "         'stop_signal_condition':'SS',\n",
    "         'cue-task_condition':'TASK',\n",
    "         'cue_condition':'CUE',\n",
    "         'cued_condition':'CUE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_id=['A3QAHF4UUBM7ZO',\n",
    "'AIQT0DPRTXYYD',\n",
    "'A3V1ZYXBJYCIIE',\n",
    "'A2WYCY1FMQOD5F',\n",
    "'A2S4YDJ9UGAXFQ',\n",
    "'A3G55RJTW3BSGM',\n",
    "'A1OSRAPSRT934Z',\n",
    "'AVJUIF9QHQRY8',\n",
    "'A1VCAMP3XM62R4',\n",
    "'A3IW9415ZOO0EX',\n",
    "'A55CXM7QR7R0N',\n",
    "'A2XUADP5L61HQ5',\n",
    "'AQL960O0LTRI8',\n",
    "'A1DS5O8MSI3ZH0',\n",
    "'A2YTO4EY3MNYAJ',\n",
    "'AY7WPVKHVNBLG',\n",
    "'A2OFN0A5CPLH57',\n",
    "'A2JI5RNPPXE8QE',\n",
    "'A1TS2SKXPX7ZED',\n",
    "'A1YC558J4E5KZ',\n",
    "'A2DWPP1KKAY0HG',\n",
    "'AD7CUW86FWEKT',\n",
    "'A2SIKW18T2DYYW',\n",
    "'AQJWO4YPR3LUQ',\n",
    "'A1L1SQ488YCCFJ',\n",
    "'A3ISDIYTS02E8C',\n",
    "'A3QJJR5Y3XE92N',\n",
    "'A1R0689JPSQ3OF',\n",
    "'A37EV8RZ82WT8E',\n",
    "'A3GEHH49HNJM57',\n",
    "'ASTR3EPUOKEXV',\n",
    "'AVMIXXCHPD291',\n",
    "'A2581F7TDPAMBQ',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check for inconsistent naming among tasks and missing of necessary columns.\n",
    "raw_dir = '/Users/tonyb/Desktop/all_data/'\n",
    "if not os.path.exists(raw_dir + 'processed'):\n",
    "    os.makedirs(raw_dir + 'processed')\n",
    "\n",
    "task_dfs = defaultdict(pd.DataFrame)\n",
    "predictive = []\n",
    "directed = []\n",
    "currenttrial = []\n",
    "task = []\n",
    "for subj_file in glob(raw_dir + 'A*/*'):\n",
    "    filey = os.path.basename(subj_file) #filey is now cued_task_switching_with_directed_forgetting_A2YBZFSUD5OP7W.csv or any other subid_taskid.csv\n",
    "    if filey.split('_')[-1].split('.')[0] in explore_id:\n",
    "        cleaned_file_name = '_cleaned.'.join(filey.split('.')) # cleaned_file_name = 's061_ANT_cleaned.csv'\n",
    "        cleaned_file_path = os.path.join(raw_dir + 'processed', cleaned_file_name) #os.path.join() adds exactly one directory separator (/) b/w  raw_dir + 'processed and cleaned_file_name\n",
    "        df = pd.read_csv(subj_file)\n",
    "        if 'exp_id' in df.columns:\n",
    "            exp_id = df.exp_id.unique()[0] #for the network_attack battery, all rows in the exp_id column has the same value\n",
    "        else:\n",
    "            exp_id = '_'.join(os.path.basename(subj_file).split('_')[0:-1]).rstrip('.csv')\n",
    "        if 'predictive_condition' in df.columns:\n",
    "            predictive.append(exp_id)\n",
    "        if 'predictive_dimension' in df.columns:\n",
    "            predictive.append(exp_id)\n",
    "        if 'directed_condition' in df.columns:\n",
    "            directed.append(exp_id)\n",
    "        if 'current_trial' not in df.columns and 'trial_index' in df.columns:\n",
    "            currenttrial.append(exp_id)\n",
    "        if 'task_condition' in df.columns:\n",
    "            task.append(exp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate concat data from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = '/Users/tonyb/Desktop/all_data/all_demographics/'\n",
    "if not os.path.exists(raw_dir + 'processed'):\n",
    "    os.makedirs(raw_dir + 'processed')\n",
    "\n",
    "task_dfs = defaultdict(pd.DataFrame)\n",
    "for subj_file in glob(raw_dir + '*demographics*'):\n",
    "    filey = os.path.basename(subj_file) #filey is now cued_task_switching_with_directed_forgetting_A2YBZFSUD5OP7W.csv or any other taskid_subid.csv\n",
    "#     if filey.split('_')[-1].split('.')[0] in explore_id:\n",
    "    cleaned_file_name = '_cleaned.'.join(filey.split('.')) # cleaned_file_name = 's061_ANT_cleaned.csv'\n",
    "    cleaned_file_path = os.path.join(raw_dir + 'processed', cleaned_file_name) #os.path.join() adds exactly one directory separator (/) b/w  raw_dir + 'processed and cleaned_file_name\n",
    "    df = pd.read_csv(subj_file)\n",
    "    if 'exp_id' in df.columns:\n",
    "        exp_id = df.exp_id.unique()[0] #for the network_attack battery, all rows in the exp_id column has the same value\n",
    "    else:\n",
    "        exp_id = '_'.join(os.path.basename(subj_file).split('_')[0:-1]).rstrip('.csv')\n",
    "    if 'worker_id' not in df.columns:\n",
    "        df.loc[:,'worker_id'] = filey.split('_')[-1].split('.')[0] #filey.split('_')[-1] returns 'A2581F7TDPAMBQ.csv'; filey.split('_')[-1].split('.')[0] returns 'A2581F7TDPAMBQ'\n",
    "    # really avoid any malfunctioning correct_trial\n",
    "    if 'correct_trial' in df.columns and 'key_press'in df.columns and 'correct_response' in df.columns:\n",
    "        df.rename(columns={'correct_trial':'original_correcttrial'},inplace=True)\n",
    "        df['correct_trial'] = np.where(df['key_press']==df['correct_response'],1,0)\n",
    "    # post process data, drop rows, etc.....\n",
    "    for colname in df.columns:\n",
    "        if 'predictive_condition' in colname:\n",
    "            df.rename(columns={colname:'predictable_condition'},inplace=True)\n",
    "            print('predictive_condition' + ' in df.columns; ' + ' ' + filey)\n",
    "        if 'predictive_dimension' in colname:\n",
    "            df.rename(columns={colname:'predictable_dimension'},inplace=True)\n",
    "            print('predictive_dimension' + ' in df.columns; ' + ' ' + filey)\n",
    "        if 'directed_condition' in colname:\n",
    "            df.rename(columns={colname:'directed_forgetting_condition'},inplace=True)\n",
    "            print('directed_condition' + ' in df.columns; ' + ' ' + filey)\n",
    "        if 'task_condition' in colname:\n",
    "            df.rename(columns={colname:'cue-task_condition'},inplace=True)\n",
    "        if ('SS' not in colname) and ('delay' in colname):\n",
    "            df.rename(columns={colname:'delay_condition'},inplace=True)\n",
    "#                                    \\'n_back_condition':'n_back_trial_type'},inplace=True)\n",
    "    drop_columns = ['Unnamed: 0','final_accuracy','final_avg_rt', 'final_credit_var','final_missed_percent',\n",
    "                    'final_responses_ok','internal_node_id','responses','stimulus',\n",
    "                    'text','trial_index','trial_type','view_history']\n",
    "    df.drop(drop_columns, axis=1, inplace=True, errors='ignore')\n",
    "    keep_dict = {'trial_id': ['test_trial']}\n",
    "    if 'trial_id' in df.columns:\n",
    "        for row, vals in keep_dict.items():\n",
    "            df = df.query('%s in %s' % (row, vals))\n",
    "    if 'predictable' in exp_id or 'predictive' in exp_id:\n",
    "        if 'current_trial' in df.columns:\n",
    "            df = df.query('current_trial > 1') #remove first trial for all predictable-task switching tasks\n",
    "        elif 'current_trial' not in df.columns:\n",
    "            if 'cue-task_condition' in df.columns:\n",
    "                df = df[-((pd.isna(df['cue-task_condition'])) | (df['cue-task_condition']==\"na\"))]\n",
    "            elif 'predictable_condition' in df.columns:\n",
    "                df = df[-((pd.isna(df['predictable_condition'])) | (df['predictable_condition']==\"na\"))]\n",
    "            elif 'predictive_condition' in df.columns:\n",
    "                df = df[-((pd.isna(df['predictive_condition'])) | (df['predictive_condition']==\"na\"))]\n",
    "    if 'n_back' in exp_id and 'current_trial' in df.columns:\n",
    "        df = df.query('current_trial > 3') #remove first 3 trials for all n_back tasks        \n",
    "    if 'cue' in exp_id and 'cue-task_condition' in df.columns:\n",
    "        df = df[-((pd.isna(df['cue-task_condition'])) | (df['cue-task_condition']==\"na\"))] #remove first trial, 'na' or null, for all cued-task switching tasks\n",
    "#     df.to_csv(cleaned_file_path, index=False)\n",
    "    task_dfs[exp_id] = pd.concat([task_dfs[exp_id], df], axis=0)\n",
    "\n",
    "# if not os.path.exists(raw_dir + 'concat_data/explore'):\n",
    "#     os.makedirs(raw_dir + 'concat_data/explore')\n",
    "for task,df in task_dfs.items():\n",
    "    if 'demographics' not in task:\n",
    "        df.to_csv(raw_dir + 'concat_data/explore/%s.csv' % task, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tasks with no 'current_trial' column:\n",
    "1. demographics_survey__stanford__nih\n",
    "2. directed_forgetting_single_task_network\n",
    "3. shape_matching_with_cued_task_switching\n",
    "4. shape_matching_with_predictable_task_switching\n",
    "5. shape_matching_single_task_network\n",
    "6. cued_task_switching_single_task_network\n",
    "7. go_nogo_single_task_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### check which tasks had malfunctioning correct_trial (that's automated by javascript)\n",
    "concat_dir = '/Users/tonyb/Desktop/all_data/'\n",
    "\n",
    "all_conditions=[]\n",
    "all_workers=[]\n",
    "for concat_file in glob(concat_dir + 'concat_data/explore/*'):\n",
    "    filey = os.path.basename(concat_file) #filey is now cued_task_switching_with_directed_forgetting.csv or any other subid_taskid.csv\n",
    "    concat_df = pd.read_csv(concat_file)\n",
    "    if 'correct_trial' in df.columns and 'original_correcttrial'in df.columns:\n",
    "        new_mean = stats.mean(concat_df['correct_trial'])\n",
    "        original_mean = stats.mean(concat_df['original_correcttrial'])\n",
    "        if new_mean != original_mean:\n",
    "            print(filey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_dir = '/Users/tonyb/Desktop/all_data/'\n",
    "taskDict = {}\n",
    "for concat_file in glob(concat_dir + 'concat_data/explore/*'):\n",
    "    filey = os.path.basename(concat_file) #filey is now cued_task_switching_with_directed_forgetting_A2YBZFSUD5OP7W.csv or any other taskid_subid.csv\n",
    "    concat_df = pd.read_csv(concat_file)\n",
    "    if 'exp_id' in concat_df.columns:\n",
    "        exp_id = concat_df.exp_id.unique()[0] #for the network_attack battery, all rows in the exp_id column has the same value\n",
    "    else:\n",
    "        print(filey + \" doesn't have an exp_id column\")\n",
    "#     print(exp_id)\n",
    "    # collapsing across H and F in flanker_condition\n",
    "    if ('flanker' in exp_id) and (len(concat_df['flanker_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'flanker_condition':[r'(^.*inc.*$)',r'(^.*_con.*$)']},\\\n",
    "                          {'flanker_condition':['incongruent','congruent']},regex=True)\n",
    "    # collapsing 7 levels in shape_matching_condition: noise-response-incompatible (i.e. distractor != target, or D in second letter) vs. no-noise (i.e. no distractor, or N in second letter); also, disregarding SSS & DSD\n",
    "    if ('shape' in exp_id) and (len(concat_df['shape_matching_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SDD','DDS','DDD']},\\\n",
    "                          {'shape_matching_condition':['T!=D','T!=D','T!=D']})\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SNN','DNN']},\\\n",
    "                          {'shape_matching_condition':['NoD','NoD']})\n",
    "    conditions = [i for i in concat_df.columns if ('condition' in i) and ('n_back_condition' not in i)]\n",
    "    conditions.sort()\n",
    "    for cond in conditions:\n",
    "        values = concat_df[cond].unique()\n",
    "        values.sort()\n",
    "        taskDict.update({abbrev[cond]:values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taskDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate descriptive stats (qa files) from concat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "concat_dir = '/Users/tonyb/Desktop/all_data/'\n",
    "\n",
    "all_conditions=[]\n",
    "all_workers=[]\n",
    "test = []\n",
    "for concat_file in glob(concat_dir + 'concat_data/explore/*'):\n",
    "    filey = os.path.basename(concat_file) #filey is now cued_task_switching_with_directed_forgetting_A2YBZFSUD5OP7W.csv or any other taskid_subid.csv\n",
    "    concat_df = pd.read_csv(concat_file)\n",
    "    if 'exp_id' in concat_df.columns:\n",
    "        exp_id = concat_df.exp_id.unique()[0] #for the network_attack battery, all rows in the exp_id column has the same value\n",
    "    else:\n",
    "        print(filey + \" doesn't have an exp_id column\")\n",
    "    print(exp_id)\n",
    "    if 'worker_id' in concat_df.columns and 'correct_trial' in concat_df.columns:\n",
    "        if len(concat_df.worker_id.unique()) > 38:\n",
    "            print(str(len(concat_df.worker_id.unique())) + ' id(s) in worker_id.')\n",
    "        elif len(concat_df.correct_trial.unique()) > 2: \n",
    "            print('     ' + str(concat_df.correct_trial.unique()) + ' are unique values in correct_trial.')\n",
    "    # collapsing across H and F in flanker_condition\n",
    "    if ('flanker' in exp_id) and (len(concat_df['flanker_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'flanker_condition':[r'(^.*inc.*$)',r'(^.*_con.*$)']},\\\n",
    "                          {'flanker_condition':['incongruent','congruent']},regex=True)\n",
    "    # collapsing 7 levels in shape_matching_condition: noise-response-incompatible (i.e. distractor != target, or D in second letter) vs. no-noise (i.e. no distractor, or N in second letter); also, disregarding SSS & DSD\n",
    "    if ('shape' in exp_id) and (len(concat_df['shape_matching_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SDD','DDS','DDD','D!=T']},\\\n",
    "                          {'shape_matching_condition':['DISTRACTOR','DISTRACTOR','DISTRACTOR','DISTRACTOR']})\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SNN','DNN','NoD']},\\\n",
    "                          {'shape_matching_condition':['CONTROL','CONTROL','CONTROL']})\n",
    "    ###generating colnames for qa_df\n",
    "    qa_colnames = []\n",
    "#     conditions = [i for i in concat_df.columns if ('condition' in i)]\n",
    "    conditions = [i for i in concat_df.columns if ('condition' in i) and ('n_back_condition' not in i)]\n",
    "    conditions.sort()\n",
    "    print(conditions)\n",
    "#     for i in conditions:\n",
    "#         if len(concat_df[i].unique()) >2: \n",
    "#             print(exp_id)\n",
    "#             print(concat_df[i].unique())\n",
    "    # in preparation for calculation of ssrt: replace and rank GoRT:\n",
    "    if ('stop' in exp_id) and ('stop_signal_condition' in concat_df.columns): #only for stop-signal tasks\n",
    "        concat_df['GoRTWithReplacement'] = np.where(concat_df['stop_signal_condition']=='go',np.where(concat_df['rt']== -1,2000,concat_df['rt']), np.nan)\n",
    "        concat_df['GoRTWithReplacementRank'] = concat_df.groupby(['worker_id']+conditions, sort=False)['GoRTWithReplacement'].rank(ascending=True, method='first')\n",
    "        concat_df.drop([i for i in concat_df.columns if 'Unnamed' in i], axis=1, inplace=True, errors='ignore')\n",
    "        concat_df.to_csv(concat_file,index=False)\n",
    "    \n",
    "    \n",
    "    ### Tasks with 1 condition variable\n",
    "    if len(conditions) == 1:\n",
    "        ordered_condition0 = concat_df[conditions[0]].unique()\n",
    "        ordered_condition0.sort()\n",
    "        for condition0 in ordered_condition0:\n",
    "            condition_name = str(abbrev[conditions[0]] + ':' + str(condition0)) #TASK:stay\n",
    "            all_conditions.append(condition_name)\n",
    "            if 'SS:go' in condition_name:\n",
    "                qa_colnames.append(str('countGoTrials_' + condition_name))\n",
    "            elif 'SS:stop' in condition_name:\n",
    "                for i in ['ssd_', 'ssrt_', 'm(stopfail_rt)_', 'sd(stopfail_rt)_', 'stopfail_rate_']:\n",
    "                    qa_colname = str(i + condition_name)\n",
    "                    qa_colnames.append(qa_colname)\n",
    "            elif 'GNG:nogo' in condition_name:\n",
    "                for i in ['m(nogofail_rt)_', 'sd(nogofail_rt)_', 'nogofail_rate_']:\n",
    "                    qa_colname = str(i + condition_name)\n",
    "                    qa_colnames.append(qa_colname)\n",
    "            else:\n",
    "                for i in ['m(rt)_','sd(rt)_','acc_']:\n",
    "                    qa_colname = str(i + condition_name) #m(rt)_TASK:stay\n",
    "                    qa_colnames.append(qa_colname)\n",
    "        qa_df = pd.DataFrame(index=concat_df.worker_id.unique(),columns=qa_colnames)\n",
    "        for condition0 in ordered_condition0:\n",
    "            condition_name = str(abbrev[conditions[0]] + ':' + str(condition0))\n",
    "            for worker_id in qa_df.index:\n",
    "                if worker_id not in all_workers:\n",
    "                    all_workers.append(worker_id)\n",
    "                ### df by worker_id to calculate descriptive stats\n",
    "                df_subj = concat_df[(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0)]\n",
    "                ### number of go trials\n",
    "                if ('stop' in exp_id) and ('SS:go' in condition_name):\n",
    "                    countGoTrials = str('countGoTrials_'+condition_name)\n",
    "                    qa_df.at[worker_id,countGoTrials] = len(df_subj) #len(df_subj): all Go trials \n",
    "                ### mean response times\n",
    "                mrt = str('m(rt)_'+condition_name)\n",
    "                if len(df_subj['rt'][(df_subj['correct_trial']==1)])==0:\n",
    "#                     print(worker_id + ' had 0 correct_trial in ' + condition_name + ' in ' + exp_id)\n",
    "                    qa_df.at[worker_id,mrt] = np.nan\n",
    "                else:\n",
    "                    mean_rt = stats.mean(df_subj['rt'][(df_subj['correct_trial']==1)])\n",
    "                    qa_df.at[worker_id,mrt] = round(mean_rt,2)\n",
    "                ### sd response times\n",
    "                sdrt = str('sd(rt)_'+condition_name)\n",
    "                if len(df_subj['rt'][(df_subj['correct_trial']==1)])<2:\n",
    "#                     print(worker_id + ' had <2 correct_trial in ' + condition_name + ' in ' + exp_id)\n",
    "                    qa_df.at[worker_id,sdrt] = np.nan\n",
    "                else:\n",
    "                    sd_rt = stats.stdev(df_subj['rt'][(df_subj['correct_trial']==1)])\n",
    "                    qa_df.at[worker_id,sdrt] = round(sd_rt,2)\n",
    "                ### accuracy rate, i.e., count of correct responses\n",
    "                accuracy = len(df_subj[df_subj['correct_trial']==1])/len(df_subj)\n",
    "                acc = str('acc_'+condition_name)\n",
    "                qa_df.at[worker_id,acc] = round(accuracy,2)\n",
    "                if ('_no' in exp_id) and (condition0 == 'nogo'):\n",
    "                    if 'go_nogo_condition' in df_subj.columns:\n",
    "                        df_subj_nogo = df_subj[(df_subj['go_nogo_condition']=='nogo')]\n",
    "                        # stopfail rate\n",
    "                        nogofail = len(df_subj_nogo[df_subj_nogo['correct_trial']==0])/len(df_subj_nogo)\n",
    "                        nogo_fail = str('nogofail_rate_'+condition_name)\n",
    "                        qa_df.at[worker_id,nogo_fail] = round(nogofail,2)\n",
    "                        #m(rt_nogofail)\n",
    "                        mrt_nogofail = str('m(nogofail_rt)_'+condition_name)    \n",
    "                        if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])==0:\n",
    "#                             print(worker_id + ' had 0 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,mrt_nogofail] = np.nan\n",
    "                        else:\n",
    "                            mrtfail = stats.mean(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])\n",
    "                            qa_df.at[worker_id,mrt_nogofail] = round(mrtfail,2)\n",
    "                        #sd(rt_nogofail)\n",
    "                        sdrt_nogofail = str('sd(nogofail_rt)_'+condition_name)\n",
    "                        if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])<2:\n",
    "#                             print(worker_id + ' had <2 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,sdrt_nogofail] = np.nan\n",
    "                        else:\n",
    "                            sdrtfail = stats.stdev(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])\n",
    "                            qa_df.at[worker_id,sdrt_nogofail] = round(sdrtfail,2)\n",
    "                    else:\n",
    "                        print('go_nogo_condition NOT in '+exp_id+'.columns')\n",
    "                if ('stop' in exp_id) and (condition0 == 'stop'):\n",
    "                    if 'stop_signal_condition' in df_subj.columns and 'SS_delay' in df_subj.columns:\n",
    "                        df_subj_stop = df_subj[(df_subj['stop_signal_condition']=='stop')]\n",
    "                        # ssd\n",
    "                        ssd = stats.mean(df_subj_stop['SS_delay'])\n",
    "                        SS_delay = str('ssd_'+condition_name)\n",
    "                        qa_df.at[worker_id,SS_delay] = round(ssd,2)\n",
    "                        # stopfail rate\n",
    "                        stopfail = len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop)\n",
    "                        stop_fail = str('stopfail_rate_'+condition_name)\n",
    "                        qa_df.at[worker_id,stop_fail] = round(stopfail,2)\n",
    "                        #ssrt\n",
    "                        rank = int(qa_df.at[worker_id,[i for i in qa_df.columns if ('countGoTrials_' in i) & ('SS:go' in i)][0]]*stopfail)\n",
    "                        nthRT = list(concat_df['GoRTWithReplacement'][(concat_df['worker_id']==worker_id) & (concat_df['GoRTWithReplacementRank']==rank)])[0]\n",
    "                        ssrt = nthRT - ssd\n",
    "                        SSRT = str('ssrt_'+condition_name)\n",
    "                        qa_df.at[worker_id,SSRT] = round(ssrt,2)\n",
    "                        #m(rt_stopfail)\n",
    "                        mrt_stopfail = str('m(stopfail_rt)_'+condition_name)    \n",
    "                        if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])==0:\n",
    "#                             print(worker_id + ' had 0 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,mrt_stopfail] = np.nan\n",
    "                        else:\n",
    "                            mrtfail = stats.mean(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])\n",
    "                            qa_df.at[worker_id,mrt_stopfail] = round(mrtfail,2)\n",
    "                        #sd(rt_stopfail)\n",
    "                        sdrt_stopfail = str('sd(stopfail_rt)_'+condition_name)\n",
    "                        if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])<2:\n",
    "#                             print(worker_id + ' had <2 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,sdrt_stopfail] = np.nan\n",
    "                        else:\n",
    "                            sdrtfail = stats.stdev(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])\n",
    "                            qa_df.at[worker_id,sdrt_stopfail] = round(sdrtfail,2)\n",
    "#                             #acc_stopfail\n",
    "#                             accfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop)*100,2)\n",
    "#                             accrt_stopfail = str('acc(stopfail)_)_'+condition_name)\n",
    "#                             qa_df.at[worker_id,accrt_stopfail] = accfail\n",
    "                    else:\n",
    "                        print('stop_signal_condition and SS_delay NOT in '+exp_id+'.columns')\n",
    "    \n",
    "    \n",
    "    ### Tasks with 2 condition variables\n",
    "    elif len(conditions) == 2:\n",
    "        ordered_condition0 = concat_df[conditions[0]].unique()\n",
    "        ordered_condition0.sort()\n",
    "        ordered_condition1 = concat_df[conditions[1]].unique()\n",
    "        ordered_condition1.sort()\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1)) #TASK:stay_&_GNG:go\n",
    "                all_conditions.append(condition_name)\n",
    "                if 'SS:go' in condition_name:\n",
    "                    qa_colnames.append(str('countGoTrials_' + condition_name))\n",
    "                elif 'SS:stop' in condition_name:\n",
    "                    for i in ['ssd_', 'ssrt_', 'm(stopfail_rt)_', 'sd(stopfail_rt)_', 'stopfail_rate_']:\n",
    "                        qa_colname = str(i + condition_name)\n",
    "                        qa_colnames.append(qa_colname)\n",
    "                elif 'GNG:nogo' in condition_name:\n",
    "                    for i in ['m(nogofail_rt)_', 'sd(nogofail_rt)_', 'nogofail_rate_']:\n",
    "                        qa_colname = str(i + condition_name)\n",
    "                        qa_colnames.append(qa_colname)\n",
    "                else:\n",
    "                    for i in ['m(rt)_','sd(rt)_','acc_']:\n",
    "                        qa_colname = str(i + condition_name) #m(rt)_TASK:stay\n",
    "                        qa_colnames.append(qa_colname)\n",
    "        qa_df = pd.DataFrame(index=concat_df.worker_id.unique(),columns=qa_colnames)\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                if 'stop' not in conditions[0]:\n",
    "                    no_stop_condition_name = str(abbrev[conditions[0]] + ':' + str(condition0))\n",
    "                else:\n",
    "                    print('stop_signal_condition NOT last in conditions')\n",
    "                condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1)) #TASK:stay_&_GNG:go\n",
    "                for worker_id in qa_df.index:\n",
    "                    if worker_id not in all_workers:\n",
    "                        all_workers.append(worker_id)\n",
    "                    df_subj = concat_df[(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df[conditions[1]]==condition1)]\n",
    "                    # number of go trials\n",
    "                    if ('stop' in exp_id) and ('SS:go' in condition_name):\n",
    "                        countGoTrials = str('countGoTrials_'+condition_name)\n",
    "                        qa_df.at[worker_id,countGoTrials] = len(df_subj)  #len(df_subj): all Go trials in one of the other condition's levels\n",
    "                    # mean response times\n",
    "                    mrt = str('m(rt)_'+condition_name)\n",
    "                    if len(df_subj['rt'][(df_subj['correct_trial']==1)])==0:\n",
    "#                         print(worker_id + ' had 0 correct trial in ' + condition_name + ' in ' + exp_id)\n",
    "                        qa_df.at[worker_id,mrt] = np.nan\n",
    "                    else:\n",
    "                        mean_rt = round(stats.mean(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                        qa_df.at[worker_id,mrt] = mean_rt\n",
    "                    # sd response times\n",
    "                    sdrt = str('sd(rt)_'+condition_name)\n",
    "                    if len(df_subj['rt'][(df_subj['correct_trial']==1)])<2:\n",
    "#                         print(worker_id + ' had <2 correct_trial in ' + condition_name + ' in ' + exp_id)\n",
    "                        qa_df.at[worker_id,sdrt] = np.nan\n",
    "                    else:\n",
    "                        sd_rt = round(stats.stdev(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                        qa_df.at[worker_id,sdrt] = sd_rt\n",
    "                    # accuracy rate, i.e., count of correct responses\n",
    "                    if len(df_subj) !=0:\n",
    "                        accuracy = round(len(df_subj[df_subj['correct_trial']==1])/len(df_subj),2)\n",
    "                        acc = str('acc_'+condition_name)\n",
    "                        qa_df.at[worker_id,acc] = accuracy\n",
    "#                     else:\n",
    "#                         qa_df.at[worker_id,acc] = np.nan\n",
    "                    if '_no' in exp_id and condition0 == 'nogo' or condition1 == 'nogo' and 'go_nogo_condition' in df_subj.columns:\n",
    "                        df_subj_nogo = df_subj[(df_subj['go_nogo_condition']=='nogo')]\n",
    "                        # nogofail rate\n",
    "                        if len(df_subj_nogo) !=0:\n",
    "                            nogofail = round(len(df_subj_nogo[df_subj_nogo['correct_trial']==0])/len(df_subj_nogo),2)\n",
    "                            nogo_fail = str('nogofail_rate_'+condition_name)\n",
    "                            qa_df.at[worker_id,nogo_fail] = nogofail\n",
    "                        else:\n",
    "                            qa_df.at[worker_id,nogo_fail] = np.nan\n",
    "                        #m(rt_nogofail)\n",
    "                        mrt_nogofail = str('m(nogofail_rt)_'+condition_name)\n",
    "                        if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])==0:\n",
    "#                             print(worker_id + ' had 0 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,mrt_nogofail] = np.nan\n",
    "                        else:\n",
    "                            mrtfail = round(stats.mean(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                            qa_df.at[worker_id,mrt_nogofail] = mrtfail\n",
    "                        #sd(rt_nogofail)\n",
    "                        sdrt_nogofail = str('sd(nogofail_rt)_'+condition_name)\n",
    "                        if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])<2:\n",
    "#                             print(worker_id + ' had <2 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,sdrt_nogofail] = np.nan\n",
    "                        else:\n",
    "                            sdrtfail = round(stats.stdev(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                            qa_df.at[worker_id,sdrt_nogofail] = sdrtfail\n",
    "                    if 'stop' in exp_id and condition0 == 'stop' or condition1 == 'stop' and 'stop_signal_condition' in df_subj.columns and 'SS_delay' in df_subj.columns:\n",
    "                        df_subj_stop = df_subj[(df_subj['stop_signal_condition']=='stop')]\n",
    "                        # ssd\n",
    "                        if len(df_subj_stop['SS_delay']) >1:\n",
    "                            ssd = round(stats.mean(df_subj_stop['SS_delay']),2)\n",
    "                            SS_delay = str('ssd_'+condition_name)\n",
    "                            qa_df.at[worker_id,SS_delay] = ssd\n",
    "                        else:\n",
    "                            qa_df.at[worker_id,SS_delay] = np.nan\n",
    "                        # stopfail rate\n",
    "                        if len(df_subj_stop) !=0:\n",
    "                            stopfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop),2)\n",
    "                            stop_fail = str('stopfail_rate_'+condition_name)\n",
    "                            qa_df.at[worker_id,stop_fail] = stopfail\n",
    "                        else:\n",
    "                            qa_df.at[worker_id,stop_fail] = np.nan\n",
    "                        #ssrt\n",
    "                        SSRT = str('ssrt_'+condition_name)\n",
    "                        rank = int(qa_df.at[worker_id,[i for i in qa_df.columns if ('countGoTrials_' in i) & (no_stop_condition_name in i) & ('SS:go' in i)][0]]*stopfail)\n",
    "                        if rank >0:\n",
    "                            nthRT = list(concat_df['GoRTWithReplacement'][(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df['GoRTWithReplacementRank']==rank)])[0]\n",
    "                            ssrt = nthRT - ssd\n",
    "                            qa_df.at[worker_id,SSRT] = round(ssrt,2)\n",
    "                        else:\n",
    "                            qa_df.at[worker_id,SSRT] = 'rank <0'\n",
    "                        #m(rt_stopfail)\n",
    "                        mrt_stopfail = str('m(stopfail_rt)_'+condition_name)\n",
    "                        if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])==0:\n",
    "#                             print(worker_id + ' had 0 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,mrt_stopfail] = np.nan\n",
    "                        else:\n",
    "                            mrtfail = round(stats.mean(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                            qa_df.at[worker_id,mrt_stopfail] = mrtfail\n",
    "                        #sd(rt_stopfail)\n",
    "                        sdrt_stopfail = str('sd(stopfail_rt)_'+condition_name)\n",
    "                        if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])<2:\n",
    "#                             print(worker_id + ' had <2 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,sdrt_stopfail] = np.nan\n",
    "                        else:\n",
    "                            sdrtfail = round(stats.stdev(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                            qa_df.at[worker_id,sdrt_stopfail] = sdrtfail\n",
    "#                             #acc_stopfail\n",
    "#                             accfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop)*100,2)\n",
    "#                             accrt_stopfail = str('acc(stopfail)_)_'+condition_name)\n",
    "#                             qa_df.at[worker_id,accrt_stopfail] = accfail\n",
    "#                     else:\n",
    "#                         print('stop_signal_condition and SS_delay NOT in '+exp_id+'.columns')\n",
    "\n",
    "\n",
    "    ### DUAL-TASKs with 3 _condition variables\n",
    "    elif len(conditions) == 3:\n",
    "        ordered_condition0 = concat_df[conditions[0]].unique()\n",
    "        ordered_condition0.sort()\n",
    "        ordered_condition1 = concat_df[conditions[1]].unique()\n",
    "        ordered_condition1.sort()\n",
    "        ordered_condition2 = concat_df[conditions[2]].unique()\n",
    "        ordered_condition2.sort()\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                for condition2 in ordered_condition2:\n",
    "                    condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1) + '_&_' + abbrev[conditions[2]] + ':' + str(condition2)) #TASK:stay_&_GNG:go\n",
    "                    all_conditions.append(condition_name)\n",
    "                    if 'SS:go' in condition_name:\n",
    "                        qa_colnames.append(str('countGoTrials_' + condition_name))\n",
    "                    elif 'SS:stop' in condition_name:\n",
    "                        for i in ['ssd_', 'ssrt_', 'm(stopfail_rt)_', 'sd(stopfail_rt)_', 'stopfail_rate_']:\n",
    "                            qa_colname = str(i + condition_name)\n",
    "                            qa_colnames.append(qa_colname)\n",
    "                    elif 'GNG:nogo' in condition_name:\n",
    "                        for i in ['m(nogofail_rt)_', 'sd(nogofail_rt)_', 'nogofail_rate_']:\n",
    "                            qa_colname = str(i + condition_name)\n",
    "                            qa_colnames.append(qa_colname)\n",
    "                    else:\n",
    "                        for i in ['m(rt)_','sd(rt)_','acc_']:\n",
    "                            qa_colname = str(i + condition_name) #m(rt)_TASK:stay\n",
    "                            qa_colnames.append(qa_colname)\n",
    "        qa_df = pd.DataFrame(index=concat_df.worker_id.unique(),columns=qa_colnames)\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                for condition2 in ordered_condition2:\n",
    "                    if ('stop' not in conditions[0]) and ('stop' not in conditions[1]):\n",
    "                        no_stop_condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1))\n",
    "                    else:\n",
    "                        print('stop_signal_condition is NOT last in conditions')\n",
    "#                         no_stop_condition_name = str(abbrev[conditions[1]] + ':' + str(condition1))\n",
    "                    condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1) + '_&_' + abbrev[conditions[2]] + ':' + str(condition2)) #TASK:stay_&_GNG:go\n",
    "                    \n",
    "                    for worker_id in qa_df.index:\n",
    "                        if worker_id not in all_workers:\n",
    "                            all_workers.append(worker_id)\n",
    "                        df_subj = concat_df[(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df[conditions[1]]==condition1) & (concat_df[conditions[2]]==condition2)]\n",
    "                        # number of go trials\n",
    "                        if ('stop' in exp_id) and ('SS:go' in condition_name):\n",
    "                            countGoTrials = str('countGoTrials_'+condition_name)\n",
    "                            qa_df.at[worker_id,countGoTrials] = len(df_subj)  #len(df_subj): all Go trials in one of the other condition's levels\n",
    "                        # mean response times\n",
    "                        mrt = str('m(rt)_'+condition_name)\n",
    "                        if len(df_subj['rt'][(df_subj['correct_trial']==1)])==0:\n",
    "    #                         print(worker_id + ' had 0 correct trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,mrt] = np.nan\n",
    "                        else:\n",
    "                            mean_rt = round(stats.mean(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                            qa_df.at[worker_id,mrt] = mean_rt\n",
    "                        # sd response times\n",
    "                        sdrt = str('sd(rt)_'+condition_name)\n",
    "                        if len(df_subj['rt'][(df_subj['correct_trial']==1)])<2:\n",
    "    #                         print(worker_id + ' had <2 correct_trial in ' + condition_name + ' in ' + exp_id)\n",
    "                            qa_df.at[worker_id,sdrt] = np.nan\n",
    "                        else:\n",
    "                            sd_rt = round(stats.stdev(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                            qa_df.at[worker_id,sdrt] = sd_rt\n",
    "                        # accuracy rate, i.e., count of correct responses\n",
    "                        if len(df_subj) !=0:\n",
    "                            accuracy = round(len(df_subj[df_subj['correct_trial']==1])/len(df_subj),2)\n",
    "                            acc = str('acc_'+condition_name)\n",
    "                            qa_df.at[worker_id,acc] = accuracy\n",
    "#                         else:\n",
    "#                             qa_df.at[worker_id,acc] = np.nan\n",
    "                        if '_no' in exp_id and condition0 == 'nogo' or condition1 == 'nogo' or condition2 == 'nogo' and 'go_nogo_condition' in df_subj.columns:\n",
    "                            df_subj_nogo = df_subj[(df_subj['go_nogo_condition']=='nogo')]\n",
    "                            # nogofail rate\n",
    "                            if len(df_subj_nogo) !=0:\n",
    "                                nogofail = round(len(df_subj_nogo[df_subj_nogo['correct_trial']==0])/len(df_subj_nogo),2)\n",
    "                                nogo_fail = str('nogofail_rate_'+condition_name)\n",
    "                                qa_df.at[worker_id,nogo_fail] = nogofail\n",
    "#                             else:\n",
    "#                                 qa_df.at[worker_id,nogo_fail] = np.nan\n",
    "                            #m(rt_nogofail)\n",
    "                            mrt_nogofail = str('m(nogofail_rt)_'+condition_name)\n",
    "                            if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])==0:\n",
    "    #                             print(worker_id + ' had 0 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,mrt_nogofail] = np.nan\n",
    "                            else:\n",
    "                                mrtfail = round(stats.mean(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                                qa_df.at[worker_id,mrt_nogofail] = mrtfail\n",
    "                            #sd(rt_nogofail)\n",
    "                            sdrt_nogofail = str('sd(nogofail_rt)_'+condition_name)\n",
    "                            if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])<2:\n",
    "    #                             print(worker_id + ' had <2 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,sdrt_nogofail] = np.nan\n",
    "                            else:\n",
    "                                sdrtfail = round(stats.stdev(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                                qa_df.at[worker_id,sdrt_nogofail] = sdrtfail\n",
    "                        if 'stop' in exp_id and condition0 == 'stop' or condition1 == 'stop' or condition2 == 'stop' and 'stop_signal_condition' in df_subj.columns and 'SS_delay' in df_subj.columns:\n",
    "                            df_subj_stop = df_subj[(df_subj['stop_signal_condition']=='stop')]\n",
    "                            # ssd\n",
    "                            if len(df_subj_stop['SS_delay']) >1:\n",
    "                                ssd = round(stats.mean(df_subj_stop['SS_delay']),2)\n",
    "                                SS_delay = str('ssd_'+condition_name)\n",
    "                                qa_df.at[worker_id,SS_delay] = ssd\n",
    "#                             else:\n",
    "#                                 qa_df.at[worker_id,SS_delay] = len(df_subj_stop['SS_delay'])\n",
    "                            # stopfail rate\n",
    "                            if len(df_subj_stop) !=0:\n",
    "                                stopfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop),2)\n",
    "                                stop_fail = str('stopfail_rate_'+condition_name)\n",
    "                                qa_df.at[worker_id,stop_fail] = stopfail\n",
    "#                             else:\n",
    "#                                 qa_df.at[worker_id,stop_fail] = np.nan\n",
    "                            #ssrt\n",
    "                            SSRT = str('ssrt_'+condition_name)\n",
    "                            rank = int(qa_df.at[worker_id,[i for i in qa_df.columns if ('countGoTrials_' in i) & (no_stop_condition_name in i) & ('SS:go' in i)][0]]*stopfail)\n",
    "                            if rank >0:\n",
    "                                nthRT = list(concat_df['GoRTWithReplacement'][(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df[conditions[1]]==condition1) & (concat_df['GoRTWithReplacementRank']==rank)])[0]\n",
    "                                ssrt = nthRT - ssd\n",
    "                                qa_df.at[worker_id,SSRT] = round(ssrt,2)\n",
    "                            else:\n",
    "                                qa_df.at[worker_id,SSRT] = 'rank <0'\n",
    "                            #m(rt_stopfail)\n",
    "                            mrt_stopfail = str('m(stopfail_rt)_'+condition_name)\n",
    "                            if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])==0:\n",
    "    #                             print(worker_id + ' had 0 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,mrt_stopfail] = np.nan\n",
    "                            else:\n",
    "                                mrtfail = round(stats.mean(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                                qa_df.at[worker_id,mrt_stopfail] = mrtfail\n",
    "                            #sd(rt_stopfail)\n",
    "                            sdrt_stopfail = str('sd(stopfail_rt)_'+condition_name)\n",
    "                            if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])<2:\n",
    "    #                             print(worker_id + ' had <2 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,sdrt_stopfail] = np.nan\n",
    "                            else:\n",
    "                                sdrtfail = round(stats.stdev(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                                qa_df.at[worker_id,sdrt_stopfail] = sdrtfail\n",
    "#                             #acc_stopfail\n",
    "#                             accfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop)*100,2)\n",
    "#                             accrt_stopfail = str('acc(stopfail)_)_'+condition_name)\n",
    "#                             qa_df.at[worker_id,accrt_stopfail] = accfail\n",
    "    #                     else:\n",
    "    #                         print('stop_signal_condition and SS_delay NOT in '+exp_id+'.columns')\n",
    "    \n",
    "    \n",
    "        ### DUAL-TASKs with 4 _condition variables\n",
    "    elif len(conditions) == 4:\n",
    "        ordered_condition0 = concat_df[conditions[0]].unique()\n",
    "        ordered_condition0.sort()\n",
    "        ordered_condition1 = concat_df[conditions[1]].unique()\n",
    "        ordered_condition1.sort()\n",
    "        ordered_condition2 = concat_df[conditions[2]].unique()\n",
    "        ordered_condition2.sort()\n",
    "        ordered_condition3 = concat_df[conditions[3]].unique()\n",
    "        ordered_condition3.sort()\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                for condition2 in ordered_condition2:\n",
    "                    for condition3 in ordered_condition3:\n",
    "                        condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1) + '_&_' + abbrev[conditions[2]] + ':' + str(condition2) + '_&_' + abbrev[conditions[3]] + ':' + str(condition3)) #TASK:stay_&_GNG:go\n",
    "                        all_conditions.append(condition_name)\n",
    "                        if 'SS:go' in condition_name:\n",
    "                            qa_colnames.append(str('countGoTrials_' + condition_name))\n",
    "                        elif 'SS:stop' in condition_name:\n",
    "                            for i in ['ssd_', 'ssrt_', 'm(stopfail_rt)_', 'sd(stopfail_rt)_', 'stopfail_rate_']:\n",
    "                                qa_colname = str(i + condition_name)\n",
    "                                qa_colnames.append(qa_colname)\n",
    "                        elif 'GNG:nogo' in condition_name:\n",
    "                            for i in ['m(nogofail_rt)_', 'sd(nogofail_rt)_', 'nogofail_rate_']:\n",
    "                                qa_colname = str(i + condition_name)\n",
    "                                qa_colnames.append(qa_colname)\n",
    "                        else:\n",
    "                            for i in ['m(rt)_','sd(rt)_','acc_']:\n",
    "                                qa_colname = str(i + condition_name) #m(rt)_TASK:stay\n",
    "                                qa_colnames.append(qa_colname)\n",
    "        qa_df = pd.DataFrame(index=concat_df.worker_id.unique(),columns=qa_colnames)\n",
    "        for condition0 in ordered_condition0:\n",
    "            for condition1 in ordered_condition1:\n",
    "                for condition2 in ordered_condition2:\n",
    "                    for condition3 in ordered_condition3:\n",
    "                        if ('stop' not in conditions[0]) and ('stop' not in conditions[1]) and ('stop' not in conditions[2]):\n",
    "                            no_stop_condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1) + '_&_' + abbrev[conditions[2]] + ':' + str(condition2))\n",
    "                        else:\n",
    "                            print('stop_signal_condition is NOT last in conditions')\n",
    "    #                         no_stop_condition_name = str(abbrev[conditions[1]] + ':' + str(condition1))\n",
    "                        condition_name = str(abbrev[conditions[0]] + ':' + str(condition0) + '_&_' + abbrev[conditions[1]] + ':' + str(condition1) + '_&_' + abbrev[conditions[2]] + ':' + str(condition2) + '_&_' + abbrev[conditions[3]] + ':' + str(condition3)) #TASK:stay_&_GNG:go\n",
    "                        for worker_id in qa_df.index:\n",
    "                            if worker_id not in all_workers:\n",
    "                                all_workers.append(worker_id)\n",
    "                            df_subj = concat_df[(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df[conditions[1]]==condition1) & (concat_df[conditions[2]]==condition2) & (concat_df[conditions[3]]==condition3)]\n",
    "                            # number of go trials\n",
    "                            if ('stop' in exp_id) and ('SS:go' in condition_name):\n",
    "                                countGoTrials = str('countGoTrials_'+condition_name)\n",
    "                                qa_df.at[worker_id,countGoTrials] = len(df_subj)  #len(df_subj): all Go trials in one of the other condition's levels\n",
    "                            # mean response times\n",
    "                            mrt = str('m(rt)_'+condition_name)\n",
    "                            if len(df_subj['rt'][(df_subj['correct_trial']==1)])==0:\n",
    "        #                         print(worker_id + ' had 0 correct trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,mrt] = np.nan\n",
    "                            else:\n",
    "                                mean_rt = round(stats.mean(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                                qa_df.at[worker_id,mrt] = mean_rt\n",
    "                            # sd response times\n",
    "                            sdrt = str('sd(rt)_'+condition_name)\n",
    "                            if len(df_subj['rt'][(df_subj['correct_trial']==1)])<2:\n",
    "        #                         print(worker_id + ' had <2 correct_trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                qa_df.at[worker_id,sdrt] = np.nan\n",
    "                            else:\n",
    "                                sd_rt = round(stats.stdev(df_subj['rt'][(df_subj['correct_trial']==1)]),2)\n",
    "                                qa_df.at[worker_id,sdrt] = sd_rt\n",
    "                            # accuracy rate, i.e., count of correct responses\n",
    "                            if len(df_subj) !=0:\n",
    "                                accuracy = round(len(df_subj[df_subj['correct_trial']==1])/len(df_subj),2)\n",
    "                                acc = str('acc_'+condition_name)\n",
    "                                qa_df.at[worker_id,acc] = accuracy\n",
    "#                             else:\n",
    "#                                 qa_df.at[worker_id,acc] = np.nan\n",
    "                            if '_no' in exp_id and condition0 == 'nogo' or condition1 == 'nogo' or condition2 == 'nogo' or condition3 == 'nogo' and 'go_nogo_condition' in df_subj.columns:\n",
    "                                df_subj_nogo = df_subj[(df_subj['go_nogo_condition']=='nogo')]\n",
    "                                # nogofail rate\n",
    "                                if len(df_subj_nogo) !=0:\n",
    "                                    nogofail = round(len(df_subj_nogo[df_subj_nogo['correct_trial']==0])/len(df_subj_nogo),2)\n",
    "                                    nogo_fail = str('nogofail_rate_'+condition_name)\n",
    "                                    qa_df.at[worker_id,nogo_fail] = nogofail\n",
    "                                else:\n",
    "                                    qa_df.at[worker_id,nogo_fail] = np.nan\n",
    "                                #m(rt_nogofail)\n",
    "                                mrt_nogofail = str('m(nogofail_rt)_'+condition_name)\n",
    "                                if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])==0:\n",
    "        #                             print(worker_id + ' had 0 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                    qa_df.at[worker_id,mrt_nogofail] = np.nan\n",
    "                                else:\n",
    "                                    mrtfail = round(stats.mean(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                                    qa_df.at[worker_id,mrt_nogofail] = mrtfail\n",
    "                                #sd(rt_nogofail)\n",
    "                                sdrt_nogofail = str('sd(nogofail_rt)_'+condition_name)\n",
    "                                if len(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)])<2:\n",
    "        #                             print(worker_id + ' had <2 nogofail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                    qa_df.at[worker_id,sdrt_nogofail] = np.nan\n",
    "                                else:\n",
    "                                    sdrtfail = round(stats.stdev(df_subj_nogo['rt'][(df_subj_nogo['correct_trial']==0)]),2)\n",
    "                                    qa_df.at[worker_id,sdrt_nogofail] = sdrtfail\n",
    "                            if 'stop' in exp_id and condition0 == 'stop' or condition1 == 'stop' or condition2 == 'stop' or condition3 == 'stop' and 'stop_signal_condition' in df_subj.columns and 'SS_delay' in df_subj.columns:\n",
    "                                df_subj_stop = df_subj[(df_subj['stop_signal_condition']=='stop')]\n",
    "                                # ssd\n",
    "                                if len(df_subj_stop['SS_delay']) >1:\n",
    "                                    ssd = round(stats.mean(df_subj_stop['SS_delay']),2)\n",
    "                                    SS_delay = str('ssd_'+condition_name)\n",
    "                                    qa_df.at[worker_id,SS_delay] = ssd\n",
    "                                else:\n",
    "                                    qa_df.at[worker_id,SS_delay] = np.nan\n",
    "                                # stopfail rate\n",
    "                                if len(df_subj_stop) !=0:\n",
    "                                    stopfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop),2)\n",
    "                                    stop_fail = str('stopfail_rate_'+condition_name)\n",
    "                                    qa_df.at[worker_id,stop_fail] = stopfail\n",
    "                                else:\n",
    "                                    qa_df.at[worker_id,stop_fail] = np.nan\n",
    "                                #ssrt\n",
    "                                SSRT = str('ssrt_'+condition_name)\n",
    "                                rank = int(qa_df.at[worker_id,[i for i in qa_df.columns if ('countGoTrials_' in i) & (no_stop_condition_name in i) & ('SS:go' in i)][0]]*stopfail)\n",
    "                                if rank >0:\n",
    "                                    nthRT = list(concat_df['GoRTWithReplacement'][(concat_df['worker_id']==worker_id) & (concat_df[conditions[0]]==condition0) & (concat_df[conditions[1]]==condition1) & (concat_df[conditions[2]]==condition2) & (concat_df['GoRTWithReplacementRank']==rank)])[0]\n",
    "                                    ssrt = nthRT - ssd\n",
    "                                    qa_df.at[worker_id,SSRT] = round(ssrt,2)\n",
    "                                else:\n",
    "                                    qa_df.at[worker_id,SSRT] = 'rank <0'\n",
    "                                #m(rt_stopfail)\n",
    "                                mrt_stopfail = str('m(stopfail_rt)_'+condition_name)\n",
    "                                if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])==0:\n",
    "        #                             print(worker_id + ' had 0 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                    qa_df.at[worker_id,mrt_stopfail] = np.nan\n",
    "                                else:\n",
    "                                    mrtfail = round(stats.mean(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                                    qa_df.at[worker_id,mrt_stopfail] = mrtfail\n",
    "                                #sd(rt_stopfail)\n",
    "                                sdrt_stopfail = str('sd(stopfail_rt)_'+condition_name)\n",
    "                                if len(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)])<2:\n",
    "        #                             print(worker_id + ' had <2 stopfail trial in ' + condition_name + ' in ' + exp_id)\n",
    "                                    qa_df.at[worker_id,sdrt_stopfail] = np.nan\n",
    "                                else:\n",
    "                                    sdrtfail = round(stats.stdev(df_subj_stop['rt'][(df_subj_stop['correct_trial']==0)]),2)\n",
    "                                    qa_df.at[worker_id,sdrt_stopfail] = sdrtfail\n",
    "        #                             #acc_stopfail\n",
    "        #                             accfail = round(len(df_subj_stop[df_subj_stop['correct_trial']==0])/len(df_subj_stop)*100,2)\n",
    "        #                             accrt_stopfail = str('acc(stopfail)_)_'+condition_name)\n",
    "        #                             qa_df.at[worker_id,accrt_stopfail] = accfail\n",
    "        #                     else:\n",
    "        #                         print('stop_signal_condition and SS_delay NOT in '+exp_id+'.columns')\n",
    "    qa_df.drop([i for i in qa_df.columns if ('countGoTrials' in i) & ('SS:stop' in i)], axis=1, inplace=True, errors='ignore')\n",
    "    qa_df.drop([i for i in qa_df.columns if ('TASK:switch' in i) & ('CUE:stay' in i)], axis=1, inplace=True, errors='ignore')\n",
    "    qa_df.drop([i for i in qa_df.columns if (('m(rt)' in i)|('sd(rt)' in i)|('acc_' in i)) & (('SS:stop' in i)|('GNG:nogo' in i))], axis=1, inplace=True, errors='ignore')\n",
    "    if ('flanker_with_cued_task_switching' in exp_id) or ('shape_matching_with_cued_task_switching' in exp_id):\n",
    "        qa_df.columns = qa_df.columns.str.replace(r'switch_new','switch')\n",
    "#     qa_df.drop([i for i in qa_df.columns if 'DF:pos' in i], axis=1, inplace=True, errors='ignore')\n",
    "    qa_df.drop([i for i in qa_df.columns if ('SSS' in i) | ('DSD' in i)], axis=1, inplace=True, errors='ignore')\n",
    "    if not os.path.exists(concat_dir + 'QA/explore'): os.makedirs(concat_dir + 'QA/explore')\n",
    "    qa_df.loc['mean'] = round(qa_df.mean(),2)\n",
    "#     qa_df.to_csv(concat_dir + 'QA/explore/' + exp_id + '_qa.csv')\n",
    "    qa_df.to_csv('~/Documents/Networking_Pilot/explore_n33/QA/' + exp_id + '_qa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###concat all main_DVs tables to one file (after a couple of cleanings-up)\n",
    "concat_mainDVs = pd.DataFrame()\n",
    "for main_DVs_file in glob('/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/csv_copy/*'):\n",
    "    DVs_filename = os.path.basename(main_DVs_file)\n",
    "    DVs_df = pd.read_csv(main_DVs_file)\n",
    "    #make col names consistent\n",
    "    # list of (old, new)\n",
    "    #for i in list, replace i[0] with i[1]\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('ts','TS')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('TScost','TS-cost')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('FE','flanker')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('fe','flanker')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('df','DF')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('Df','DF')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('commission','GNG-commit')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('ssrt','SSRT')\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('Unnamed: 0','explore_id')\n",
    "    #specify TS-cost from predictable_ or cued_task-switching\n",
    "    if 'predict' in DVs_filename and 'cued' not in DVs_filename:\n",
    "        DVs_df.columns = DVs_df.columns.str.replace('TS','predict-TS')\n",
    "    elif 'cued' in DVs_filename and 'predict' not in DVs_filename:\n",
    "        DVs_df.columns = DVs_df.columns.str.replace('TS','cued-TS')\n",
    "    #add the other task name to DC_ and DE_ columns\n",
    "    task1 = DVs_df.columns[1]\n",
    "    task2 = DVs_df.columns[2]\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('DC_%s' %task1, 'DC_%s_%s' %(task1,task2))\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('DE_%s' %task1, 'DE_%s_%s' %(task1,task2))\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('DC_%s' %task2, 'DC_%s_%s' %(task2,task1))\n",
    "    DVs_df.columns = DVs_df.columns.str.replace('DE_%s' %task2, 'DE_%s_%s' %(task2,task1))\n",
    "    #write main_DVs_analyses with workerId col and mean row still\n",
    "    DVs_df.to_csv('/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/csv/%s' %DVs_filename, index=False)\n",
    "    #concat all main_DVs_analyses after removing the workerId col and the mean row\n",
    "    concat_mainDVs = pd.concat([concat_mainDVs , DVs_df.iloc[:-1 , 1:]], axis=1)\n",
    "#remove duplicated cols by column name\n",
    "concat_mainDVs = concat_mainDVs.loc[:,~concat_mainDVs.columns.duplicated()]\n",
    "#reorder concat df to be clustered by tasks then write\n",
    "single_cols = [i for i in concat_mainDVs.columns if ('DC' not in i) and ('DE' not in i)]\n",
    "single_cols = sorted(single_cols,key=str.casefold)\n",
    "cued_cols = [i for i in concat_mainDVs.columns if (('cued-TS-cost' in i)&(i in single_cols)) or ('DC_cued' in i) or ('DE_cued' in i)]\n",
    "DF_cols = [i for i in concat_mainDVs.columns if (('DF' in i)&(i in single_cols)) or ('DC_DF' in i) or ('DE_DF' in i)]\n",
    "flanker_cols = [i for i in concat_mainDVs.columns if (('flanker' in i)&(i in single_cols)) or ('DC_flanker' in i) or ('DE_flanker' in i)]\n",
    "GNG_cols = [i for i in concat_mainDVs.columns if (('GNG-commit' in i)&(i in single_cols)) or ('DC_GNG-commit' in i) or ('DE_GNG-commit' in i)]\n",
    "nback_cols = [i for i in concat_mainDVs.columns if (('nback' in i)&(i in single_cols)) or ('DC_nback' in i) or ('DE_nback' in i)]\n",
    "predict_cols = [i for i in concat_mainDVs.columns if (('predict-TS-cost' in i)&(i in single_cols)) or ('DC_predict' in i) or ('DE_predict' in i)]\n",
    "shape_cols = [i for i in concat_mainDVs.columns if (('shape' in i)&(i in single_cols)) or ('DC_shape' in i) or ('DE_shape' in i)]\n",
    "stop_cols = [i for i in concat_mainDVs.columns if (('SSRT' in i)&(i in single_cols)) or ('DC_SSRT' in i) or ('DE_SSRT' in i)]\n",
    "cued_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(cued_cols)]\n",
    "DF_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(DF_cols)]\n",
    "flanker_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(flanker_cols)]\n",
    "GNG_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(GNG_cols)]\n",
    "nback_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(nback_cols)]\n",
    "predict_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(predict_cols)]\n",
    "shape_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(shape_cols)]\n",
    "stop_cols_df = concat_mainDVs.loc[:,concat_mainDVs.columns.isin(stop_cols)]\n",
    "concat_mainDVs = pd.concat([cued_cols_df,DF_cols_df,flanker_cols_df,GNG_cols_df,\n",
    "                            nback_cols_df,predict_cols_df,shape_cols_df,stop_cols_df],axis=1)\n",
    "#write concat df\n",
    "concat_mainDVs.to_csv('/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/concat/concat_mainDVs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_mainDVs_corr = round(concat_mainDVs.corr(),3)\n",
    "concat_mainDVs_corr.to_csv('/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/concat/concat_mainDVs_corr_index=False.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap using matplot\n",
    "# df = pd.read_csv('/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/concat/concat_mainDVs.csv')#.iloc[:,105:len(concat_mainDVs.columns)+1]\n",
    "# f = plt.figure(figsize=(130, 130), facecolor='w') #facecolor is background color\n",
    "# plt.matshow(df.corr(), cmap=cm.seismic, fignum=f.number, vmin=-1, vmax=1)\n",
    "# xticks = list(range(df.shape[1]))\n",
    "# xticks.insert(df.shape[1]+1,df.shape[1]-0.5)\n",
    "# yticks = list(range(df.shape[1]))\n",
    "# yticks.insert(0,-0.5)\n",
    "# yticks.insert(df.shape[1]+1,df.shape[1]-0.5)\n",
    "# plt.xticks(xticks, df.columns, fontsize=15, rotation=80)\n",
    "# plt.yticks(yticks, df.columns.insert(0,'').insert(len(df.columns)+1,''), fontsize=15)\n",
    "# for y in range(df.shape[0]):\n",
    "#     for x in range(df.shape[1]):\n",
    "#         plt.text(x + 0.5, y + 0.5, '%.3f' % data.iloc[y, x],\n",
    "#                  horizontalalignment='center',\n",
    "#                  verticalalignment='center',\n",
    "#                  )\n",
    "# cb = plt.colorbar(shrink=0.5, aspect=30, fraction=.12, pad=.02)\n",
    "# cb.ax.tick_params(labelsize=60)\n",
    "# plt.title('Main-DVs Correlation Matrix', fontsize=80);\n",
    "\n",
    "#corr data with (1) full index and col names; (2) 1's on diagonal replaced by means of corresponding raw data\n",
    "annotData = pd.read_csv(main_DVs_concat_dir + 'concat_mainDVs_corr_index=False.csv')\n",
    "annotData.index = annotData.columns\n",
    "for i in annotData.index:\n",
    "    for j in annotData.columns:\n",
    "        if i==j:\n",
    "            annotData.loc[i,j] = concat_mainDVs[j].mean()\n",
    "\n",
    "#data for drawing heatmap\n",
    "data = pd.read_csv(main_DVs_concat_dir + 'concat_mainDVs_corr_index=False.csv') \n",
    "column_labels = data.columns\n",
    "row_labels = data.columns\n",
    "\n",
    "#set up background (fig) and main map (ax)\n",
    "fig, ax = plt.subplots(figsize=(130,130), facecolor='beige')\n",
    "ax.set_title('All DV variants Corr Matrix',size=80)\n",
    "heatmap = ax.pcolor(data, cmap=cm.seismic, vmin=-1, vmax=1)\n",
    " \n",
    "# Put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(0.5,data.shape[1],1), minor=False)\n",
    "ax.set_yticks(np.arange(0.5,data.shape[0],1), minor=False)\n",
    "\n",
    "for y in range(data.shape[0]):\n",
    "    for x in range(data.shape[1]):\n",
    "        text = ax.text(x + 0.5, y + 0.5, '%.3f' % annotData.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    " \n",
    "# Want a more natural, table-like display\n",
    "ax.axis('square')\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    " \n",
    "ax.set_xticklabels(row_labels, minor=False, rotation=80, size=15)\n",
    "ax.set_yticklabels(column_labels, minor=False, size=15)\n",
    "\n",
    "# color bar\n",
    "cb = plt.colorbar(heatmap,shrink=0.5, aspect=30, fraction=.12, pad=.02)\n",
    "cb.ax.tick_params(labelsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskByTask_meanCorr_matrix = pd.DataFrame(index=single_cols,columns=single_cols)\n",
    "for i in single_cols:\n",
    "    start_index = concat_mainDVs_corr.index.get_loc(i)\n",
    "    for j in single_cols:\n",
    "        start_col = concat_mainDVs_corr.columns.get_loc(j)\n",
    "        #get the concat df\n",
    "        taskByTask_corrDf = concat_mainDVs_corr.iloc[start_index:start_index+15 , start_col:start_col+15]\n",
    "        if not os.path.exists(main_DVs_concat_dir + 'taskByTask_corr/'): os.makedirs(main_DVs_concat_dir + 'taskByTask_corr/')\n",
    "        taskByTask_corrDf.to_csv(main_DVs_concat_dir + 'taskByTask_corr/%s_by_%s.csv' %(i,j))\n",
    "        #exclude 1's on the diagonal in correlation of the same 15 variants (i.e. of the same main_DV)\n",
    "        if i==j:\n",
    "            for a in range(taskByTask_corrDf.shape[0]):\n",
    "                for b in range(taskByTask_corrDf.shape[1]):\n",
    "                    if a==b:\n",
    "                        taskByTask_corrDf.iloc[a,b] = np.nan\n",
    "        #get average of all 15x15 corr numbers\n",
    "        taskByTask_corrDf_mean = round(taskByTask_corrDf.iloc[:,:].mean().mean(),3)\n",
    "        taskByTask_meanCorr_matrix.loc[i,j] = taskByTask_corrDf_mean\n",
    "taskByTask_meanCorr_matrix.to_csv(main_DVs_concat_dir + 'taskByTask_meanCorr_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(main_DVs_concat_dir+'concat_8x8_corr.csv')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8),facecolor='beige')\n",
    "plt.title('Mean Correlation Matrix',size=14,pad=15)\n",
    "plt.pcolor(df,cmap=cm.seismic,vmin=-1,vmax=1)\n",
    "plt.yticks(np.arange(0.5, len(concat_8x8_corr.index), 1), concat_8x8_corr.index, size=11)\n",
    "plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns, rotation=70, size=11)\n",
    "plt.axis('square')\n",
    "for y in range(df.shape[0]):\n",
    "    for x in range(df.shape[1]):\n",
    "        plt.text(x + 0.5, y + 0.5, '%.3f' % df.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    "cb = plt.colorbar(shrink=0.8, aspect=12.5, fraction=.12, pad=.03)\n",
    "cb.ax.tick_params(labelsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(main_DVs_concat_dir + 'taskByTask_meanCorr_matrix.csv')\n",
    "\n",
    "column_labels = data.columns\n",
    "row_labels = concat_8x8_corr.index\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(7,7), facecolor='beige')\n",
    "ax.set_title('Mean Correlation Matrix',size=15,pad=10)\n",
    "heatmap = ax.pcolor(data, cmap=cm.seismic, vmin=-1, vmax=1)\n",
    " \n",
    "# Put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(0.5,data.shape[1],1), minor=False)\n",
    "ax.set_yticks(np.arange(0.5,data.shape[0],1), minor=False)\n",
    "\n",
    "for y in range(data.shape[0]):\n",
    "    for x in range(data.shape[1]):\n",
    "        text = ax.text(x + 0.5, y + 0.5, '%.3f' % data.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    " \n",
    "# Want a more natural, table-like display\n",
    "ax.axis('square')\n",
    "ax.invert_yaxis()\n",
    "ax.xaxis.tick_top()\n",
    " \n",
    "ax.set_xticklabels(row_labels, minor=False, rotation=70, size=11)\n",
    "ax.set_yticklabels(column_labels, minor=False, size=11)\n",
    "\n",
    "# color bar\n",
    "cb = plt.colorbar(heatmap,shrink=0.875, aspect=10.5, fraction=.12, pad=.03)\n",
    "cb.ax.tick_params(labelsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### check for all-empty columns\n",
    "for qa_file in glob('/Users/tonyb/Desktop/all_data/QA/explore/*'):\n",
    "    qa_df = pd.read_csv(qa_file)\n",
    "    for i in qa_df.columns:\n",
    "        if qa_df[i].isnull().all():\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking if any two m(rt) columns are identical and not equal to -1, which is the value for no response made\n",
    "for test_file in glob('/Users/tonyb/Desktop/all_data/QA/explore/*'):\n",
    "    test = pd.read_csv(test_file)\n",
    "#     test.columns = test.columns.droplevel(0) #if there are multiindex in columns\n",
    "    for i in range(len(test.columns)-1):\n",
    "        for j in range(i+1,len(test.columns)):\n",
    "            count = 0\n",
    "            for row in range(len(test)):\n",
    "                if test.iloc[row,i] == test.iloc[row,j] or np.isnan(test.iloc[row,i]) and np.isnan(test.iloc[row,j]): \n",
    "                    count +=1\n",
    "            if count == len(test): print('%s == %s' %(test.columns[i],test.columns[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEPRECATED!!! Kept because I'm weak...\n",
    "\n",
    "#8x8 map where each of the 8 diagonal cells is the average off of a 15x15 corr matrix among all 15 variants of each task (after removing the fifteen 1's), \n",
    "#and off-diagonal cells are averages from 30x30 matrices among 15 variants of 1 task and 15 variants of one of the other 7 tasks (after removing the fifteen 1's).\n",
    "task_dfs_list = [cued_cols_df,DF_cols_df,flanker_cols_df,GNG_cols_df,\n",
    "                 nback_cols_df,predict_cols_df,shape_cols_df,stop_cols_df]\n",
    "main_DVs_concat_dir = '/Users/tonyb/Documents/Networking_Pilot/explore_n33/main_DVs_analyses/concat/'\n",
    "if not os.path.exists(main_DVs_concat_dir + 'task_x_task/'):\n",
    "    os.makedirs(main_DVs_concat_dir + 'task_x_task/')\n",
    "if not os.path.exists(main_DVs_concat_dir + 'task_x_task_corr/'):\n",
    "    os.makedirs(main_DVs_concat_dir + 'task_x_task_corr/')\n",
    "#getting a combination of 2 out of 8 tasks with replacement so that a pair of one task with itself is also included\n",
    "combination_tasks = list(itertools.combinations_with_replacement(range(len(task_dfs_list)),2))\n",
    "#making 8x8 table\n",
    "concat_8x8_corr = pd.DataFrame(index=sorted(single_cols, key=str.casefold), columns=sorted(single_cols, key=str.casefold))\n",
    "for combination in combination_tasks:\n",
    "    task1_index = combination[0]\n",
    "    task2_index = combination[1]\n",
    "    if task1_index == task2_index: #combination of a task with itself, e.g., cued with cued\n",
    "        concat_combination_df = task_dfs_list[task1_index]\n",
    "        file_name = concat_combination_df.columns[concat_combination_df.columns.isin(single_cols)][0] #there is only one column in concat_combination_df that matches with one element of the single_cols list, so wherever that is will be used for the file name\n",
    "    else:\n",
    "        \n",
    "        concat_combination_df = pd.concat([task_dfs_list[task1_index], task_dfs_list[task2_index]], axis=1)\n",
    "        task1_name = concat_combination_df.columns[concat_combination_df.columns.isin(single_cols)][0]\n",
    "        task2_name = concat_combination_df.columns[concat_combination_df.columns.isin(single_cols)][1]\n",
    "        file_name = str(task1_name + '_by_' + task2_name)\n",
    "    concat_combination_df.to_csv(main_DVs_concat_dir + 'task_x_task/' + file_name + '.csv', index=False)\n",
    "    concat_combin_corr_df = concat_combination_df.corr().replace(1,np.nan)\n",
    "    concat_combin_corr_df.to_csv(main_DVs_concat_dir + 'task_x_task_corr/' + file_name + '_corr.csv', index=False)\n",
    "    #calculate average of all corr in the current task_x_task df\n",
    "    concat_combin_corr_mean = round(concat_combin_corr_df.iloc[:,:].mean().mean(),3)\n",
    "    if task1_index == task2_index:\n",
    "        concat_8x8_corr.loc[file_name,file_name] = concat_combin_corr_mean\n",
    "    else:\n",
    "        concat_8x8_corr.loc[task1_name,task2_name] = concat_combin_corr_mean\n",
    "        concat_8x8_corr.loc[task2_name,task1_name] = concat_combin_corr_mean\n",
    "concat_8x8_corr.to_csv(main_DVs_concat_dir + 'concat_8x8_corr.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
