{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import statistics as stats\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = '/Users/tonyb/Documents/Networking_Pilot/'\n",
    "raw_dir = project_dir + 'all_data/'\n",
    "explore_dir = project_dir + 'explore_n33/'\n",
    "main_DVs_analyses = explore_dir + 'main_DVs_analyses/'\n",
    "separate_conditions = explore_dir + 'separate_conditions/'\n",
    "\n",
    "for i in [main_DVs_analyses, separate_conditions]:\n",
    "    if not os.path.exists(i): os.makedirs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrev = {'directed_forgetting_condition':'DF',\n",
    "#           'directed_condition': 'DF',\n",
    "         'flanker_condition':'FLANKER',\n",
    "         'go_nogo_condition':'GNG',\n",
    "         'n_back_condition':'NBACK',\n",
    "          'delay_condition':'DELAY',\n",
    "         'predictable_condition':'PREDICT',\n",
    "#           'predictive_condition':'PREDICT',\n",
    "         'shape_matching_condition':'SHAPE',\n",
    "         'stop_signal_condition':'SS',\n",
    "         'cue-task_condition':'TASK',\n",
    "         'cue_condition':'CUE',\n",
    "         'cued_condition':'CUE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_id=['A3QAHF4UUBM7ZO',\n",
    "'AIQT0DPRTXYYD',\n",
    "'A3V1ZYXBJYCIIE',\n",
    "'A2WYCY1FMQOD5F',\n",
    "'A2S4YDJ9UGAXFQ',\n",
    "'A3G55RJTW3BSGM',\n",
    "'A1OSRAPSRT934Z',\n",
    "'AVJUIF9QHQRY8',\n",
    "'A1VCAMP3XM62R4',\n",
    "'A3IW9415ZOO0EX',\n",
    "'A55CXM7QR7R0N',\n",
    "'A2XUADP5L61HQ5',\n",
    "'AQL960O0LTRI8',\n",
    "'A1DS5O8MSI3ZH0',\n",
    "'A2YTO4EY3MNYAJ',\n",
    "'AY7WPVKHVNBLG',\n",
    "'A2OFN0A5CPLH57',\n",
    "'A2JI5RNPPXE8QE',\n",
    "'A1TS2SKXPX7ZED',\n",
    "'A1YC558J4E5KZ',\n",
    "'A2DWPP1KKAY0HG',\n",
    "'AD7CUW86FWEKT',\n",
    "'A2SIKW18T2DYYW',\n",
    "'AQJWO4YPR3LUQ',\n",
    "'A1L1SQ488YCCFJ',\n",
    "'A3ISDIYTS02E8C',\n",
    "'A3QJJR5Y3XE92N',\n",
    "'A1R0689JPSQ3OF',\n",
    "'A37EV8RZ82WT8E',\n",
    "'A3GEHH49HNJM57',\n",
    "'ASTR3EPUOKEXV',\n",
    "'AVMIXXCHPD291',\n",
    "'A2581F7TDPAMBQ',]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One table with all SE, DC, and DE variables (using RT or Acc; 3 steps: taskDict, individual table, one concat table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Generate a dictionary of tasks followed by their conditions and corresponding levels: {'taskname': {'dv0':[val0,val1], '...':[...,...]}}\n",
    "taskDict = {}\n",
    "for concat_file in glob(explore_dir + 'concat/*'):\n",
    "    filey = os.path.basename(concat_file).split('.')[0]\n",
    "    if 'no_go' in filey: filey = filey.replace('no_go','nogo') #SS with GNG, annoyingly, has 'go_no_go' in name instead of 'go_nogo' as in the other 7 GNG tasks\n",
    "    concat_df = pd.read_csv(concat_file)\n",
    "    exp_id = concat_df.exp_id.unique()[0] #for the network_attack battery, all rows in the exp_id column has the same value\n",
    "    # collapsing across H and F in flanker_condition\n",
    "    if ('flanker' in exp_id) and (len(concat_df['flanker_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'flanker_condition':[r'(^.*inc.*$)',r'(^.*_con.*$)']},\\\n",
    "                          {'flanker_condition':['incongruent','congruent']},regex=True)\n",
    "    # collapsing 7 levels in shape_matching_condition: noise-response-incompatible (i.e. distractor != target, or D in second letter) vs. no-noise (i.e. no distractor, or N in second letter); also, disregarding SSS & DSD\n",
    "    if ('shape' in exp_id) and (len(concat_df['shape_matching_condition'].unique())>2):\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SDD','DDS','DDD','D!=T']},\\\n",
    "                          {'shape_matching_condition':['DISTRACTOR','DISTRACTOR','DISTRACTOR','DISTRACTOR']})\n",
    "        concat_df = concat_df.replace({'shape_matching_condition':['SNN','DNN','NoD']},\\\n",
    "                          {'shape_matching_condition':['CONTROL','CONTROL','CONTROL']})\n",
    "    conditions = [i for i in concat_df.columns if ('condition' in i) and ('n_back_condition' not in i)]\n",
    "    conditions.sort()\n",
    "    val_list = []\n",
    "    for cond in conditions: #delay_condition, flanker_condition\n",
    "        values = concat_df[cond].unique() #[1,2,3]; [incongruent,congruent]\n",
    "        values.sort() #[1,2,3]; [congruent,incongruent]\n",
    "        values = values[0:2] #[1,2] considering only these two levels for now; [congruent,incongruent]\n",
    "        val_list.append({abbrev[cond]:values}) #[{DELAY:[1,2]}, {FLANKER:[congruent,incongruent]}]\n",
    "    taskDict.update({filey:val_list}) #{'n_back_with_flanker_task': [{'DELAY':[1,2]},{'FLANKER':['congruent','incongruent']}]}\n",
    "\n",
    "len(taskDict) #should equal the # of tasks (i.e., 36)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Individual tables (per task) for single effects (SE), dual-contexts (DC), dual-effects (DE), using RTs or Accuracy Rate\n",
    "for i in ['RT','Acc']:\n",
    "    new_path = main_DVs_analyses + 'mainDVs%s/' %i #path for output\n",
    "    if not os.path.exists(new_path): \n",
    "        os.makedirs(new_path)\n",
    "\n",
    "for qa_file in glob(explore_dir + 'QA/*'):\n",
    "    taskName = os.path.basename(qa_file).split('.')[0][:-3] #the last three characters are '_qa'\n",
    "    if 'no_go' in taskName: taskName = taskName.replace('no_go','nogo')\n",
    "    qaDf = pd.read_csv(qa_file, index_col=0) #index_col is workerID\n",
    "    #using RTs\n",
    "    if 'stop' in taskName: #in stop-signal tasks: m(rt)...SS:go is baseline; ssrt...SS:stop is treatment\n",
    "        qaDf = qaDf.filter(regex='m\\(rt\\)|ssrt')\n",
    "    else:\n",
    "        qaDf = qaDf.filter(regex='m\\(')\n",
    "    #using Accuracy\n",
    "#     qaDf = qaDf.filter(regex='acc') \n",
    "\n",
    "    if len(taskDict[taskName]) == 1:\n",
    "        dv0   = list(taskDict[taskName][0].keys())[0]\n",
    "        vals0 = list(taskDict[taskName][0].values())\n",
    "        \n",
    "        base0  = qaDf.filter(regex='%s' %vals0[0][0])\n",
    "        treat0 = qaDf.filter(regex='%s' %vals0[0][1])\n",
    "        \n",
    "        if 'stop' in taskName: #stop_signal_single is exception: ssrt, defined as treatment condition, already is SE.\n",
    "            SE = treat0\n",
    "        else: #for all other tasks, SE is still treatment minus baseline\n",
    "            SE = pd.DataFrame(data=treat0.iloc[:,0] - base0.iloc[:,0], columns=['_'.join(['SE',dv0])])\n",
    "        mainDvDf = SE\n",
    "    \n",
    "    elif len(taskDict[taskName]) == 2:\n",
    "        if taskName == 'stop_signal_with_go_nogo':\n",
    "            print(taskName+' has many rank<0/NaN values; no further processing for this task.')\n",
    "        else:\n",
    "            dv0   = list(taskDict[taskName][0].keys())[0]\n",
    "            vals0 = list(taskDict[taskName][0].values())\n",
    "            dv1   = list(taskDict[taskName][1].keys())[0]\n",
    "            vals1 = list(taskDict[taskName][1].values())\n",
    "            #taskDict is generated such that the baseline condition of a DV is always first (thus, [0][0]),\n",
    "            #and the treatment condition is always second (thus, [0][1])\n",
    "            base0_base1   = qaDf.filter(regex='%s' %vals0[0][0]).filter(regex='%s' %vals1[0][0])\n",
    "            base0_treat1  = qaDf.filter(regex='%s' %vals0[0][0]).filter(regex='%s' %vals1[0][1])\n",
    "            treat0_base1  = qaDf.filter(regex='%s' %vals0[0][1]).filter(regex='%s' %vals1[0][0])\n",
    "            treat0_treat1 = qaDf.filter(regex='%s' %vals0[0][1]).filter(regex='%s' %vals1[0][1])\n",
    "\n",
    "            DC_0_1 = pd.DataFrame(data=treat0_base1.iloc[:,0]  -  base0_base1.iloc[:,0], columns=['_'.join(['DC',dv0,dv1])])\n",
    "            DC_1_0 = pd.DataFrame(data=base0_treat1.iloc[:,0]  -  base0_base1.iloc[:,0], columns=['_'.join(['DC',dv1,dv0])])\n",
    "            DE_0_1 = pd.DataFrame(data=treat0_treat1.iloc[:,0] - base0_treat1.iloc[:,0], columns=['_'.join(['DE',dv0,dv1])])\n",
    "            DE_1_0 = pd.DataFrame(data=treat0_treat1.iloc[:,0] - treat0_base1.iloc[:,0], columns=['_'.join(['DE',dv1,dv0])])\n",
    "\n",
    "            mainDvDf = pd.concat([DC_0_1, DC_1_0, DE_0_1, DE_1_0],axis=1)\n",
    "    \n",
    "    elif len(taskDict[taskName]) == 3: #only cases are cuedTS tasks: 2 DVs from cuedTS (TASK & CUE); 1 from the other task\n",
    "        dv0 = 'CUE'\n",
    "        for i in range(0,3):\n",
    "            temp = list(taskDict[taskName][i].keys())[0]\n",
    "            if ('TASK' not in temp) and ('CUE' not in temp):\n",
    "                dv1 = temp\n",
    "                vals1 = list(taskDict[taskName][i].values())\n",
    "        #TASK and CUE are considered as though one DV (e.g., 0); the other DV is 1\n",
    "        base0_base1   = qaDf.filter(regex='TASK:stay_&_CUE:stay').filter(regex='%s' %vals1[0][0])\n",
    "        base0_treat1  = qaDf.filter(regex='TASK:stay_&_CUE:stay').filter(regex='%s' %vals1[0][1])\n",
    "        treat0_base1  = qaDf.filter(regex='TASK:switch_&_CUE:switch').filter(regex='%s' %vals1[0][0])\n",
    "        treat0_treat1 = qaDf.filter(regex='TASK:switch_&_CUE:switch').filter(regex='%s' %vals1[0][1])\n",
    "\n",
    "        DC_0_1 = pd.DataFrame(data=treat0_base1.iloc[:,0]  -  base0_base1.iloc[:,0], columns=['_'.join(['DC',dv0,dv1])])\n",
    "        DC_1_0 = pd.DataFrame(data=base0_treat1.iloc[:,0]  -  base0_base1.iloc[:,0], columns=['_'.join(['DC',dv1,dv0])])\n",
    "        DE_0_1 = pd.DataFrame(data=treat0_treat1.iloc[:,0] - base0_treat1.iloc[:,0], columns=['_'.join(['DE',dv0,dv1])])\n",
    "        DE_1_0 = pd.DataFrame(data=treat0_treat1.iloc[:,0] - treat0_base1.iloc[:,0], columns=['_'.join(['DE',dv1,dv0])])\n",
    "\n",
    "        mainDvDf = pd.concat([DC_0_1, DC_1_0, DE_0_1, DE_1_0],axis=1)\n",
    "    \n",
    "    elif len(taskDict[taskName]) == 4:\n",
    "    # n_back_with_cued_task_switching is the only case (from cuedTS: 'TASK','CUE'; from n-back task: 'NBACK','DELAY').\n",
    "    # If, when generating QA files (in concat_descriptive_network_attack.ipynb),\n",
    "        # conditions = [...] is defined to exclude n_back_condition, then there's none.\n",
    "    # If conditions includes n_back_condition, DELAY and NBACK should be considered as though one DV (like with TASK and CUE above)\n",
    "        print('4 DVs: taskName')\n",
    "    \n",
    "    mainDvDf.to_csv(main_DVs_analyses + 'mainDVsRT/%s_mainDVsRT.csv' %taskName)\n",
    "#     mainDvDf.to_csv(main_DVs_analyses + 'mainDVsAcc/%s_mainDVsAcc.csv' %taskName)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### one table with all SE, DC, DE data, using RTs or Accuracy \n",
    "### switch input and output directories accordingly\n",
    "concat_mainDVs = pd.DataFrame()\n",
    "for main_DVs_file in glob(main_DVs_analyses + 'mainDVsRT/*'):\n",
    "# for main_DVs_file in glob(main_DVs_analyses + 'mainDVsAcc/*'):\n",
    "    DVs_df = pd.read_csv(main_DVs_file,index_col=0)\n",
    "    #concat all main_DVs_analyses after removing the workerId col\n",
    "    concat_mainDVs = pd.concat([concat_mainDVs , DVs_df], axis=1)\n",
    "    concat_mainDVs.columns = concat_mainDVs.columns.str.replace('DELAY','NBACK')\n",
    "    \n",
    "#remove duplicated cols by column name\n",
    "concat_mainDVs = concat_mainDVs.loc[:,~concat_mainDVs.columns.duplicated()]\n",
    "\n",
    "#reorder concat df to be clustered by tasks then write\n",
    "single_cols = [i for i in concat_mainDVs.columns if ('SE' in i)]\n",
    "single_cols = sorted(single_cols,key=str.casefold)\n",
    "\n",
    "cued_cols = [i for i in concat_mainDVs.columns if ('DC_CUE' in i) or ('DE_CUE' in i)]\n",
    "DF_cols = [i for i in concat_mainDVs.columns if ('DC_DF' in i) or ('DE_DF' in i)]\n",
    "flanker_cols = [i for i in concat_mainDVs.columns if ('DC_FLANKER' in i) or ('DE_FLANKER' in i)]\n",
    "GNG_cols = [i for i in concat_mainDVs.columns if ('DC_GNG' in i) or ('DE_GNG' in i)]\n",
    "nback_cols = [i for i in concat_mainDVs.columns if ('DC_NBACK' in i) or ('DE_NBACK' in i)]\n",
    "predict_cols = [i for i in concat_mainDVs.columns if ('DC_PREDICT' in i) or ('DE_PREDICT' in i)]\n",
    "shape_cols = [i for i in concat_mainDVs.columns if ('DC_SHAPE' in i) or ('DE_SHAPE' in i)]\n",
    "stop_cols = [i for i in concat_mainDVs.columns if ('DC_SS' in i) or ('DE_SS' in i)]\n",
    "\n",
    "cued_cols_df = pd.concat([concat_mainDVs['SE_CUE'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(cued_cols)]],axis=1)\n",
    "DF_cols_df = pd.concat([concat_mainDVs['SE_DF'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(DF_cols)]],axis=1)\n",
    "flanker_cols_df = pd.concat([concat_mainDVs['SE_FLANKER'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(flanker_cols)]],axis=1)\n",
    "GNG_cols_df = pd.concat([concat_mainDVs['SE_GNG'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(GNG_cols)]],axis=1)\n",
    "nback_cols_df = pd.concat([concat_mainDVs['SE_NBACK'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(nback_cols)]],axis=1)\n",
    "predict_cols_df = pd.concat([concat_mainDVs['SE_PREDICT'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(predict_cols)]],axis=1)\n",
    "shape_cols_df = pd.concat([concat_mainDVs['SE_SHAPE'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(shape_cols)]],axis=1)\n",
    "stop_cols_df = pd.concat([concat_mainDVs['SE_SS'],concat_mainDVs.loc[:,concat_mainDVs.columns.isin(stop_cols)]],axis=1)\n",
    "\n",
    "#write concat df without the mean row\n",
    "concat_mainDVs = pd.concat([cued_cols_df,DF_cols_df,flanker_cols_df,GNG_cols_df,\n",
    "                            nback_cols_df,predict_cols_df,shape_cols_df,stop_cols_df],axis=1)\n",
    "concat_mainDVs.iloc[:-1 , :].to_csv(main_DVs_analyses+'concat/concat_mainDVsRTs.csv', index=False)\n",
    "# concat_mainDVs.iloc[:-1 , :].to_csv(main_DVs_analyses+'concat/concat_mainDVsAcc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8x8 mean DCs and DEs summary (using accuracy or RT as main DV, depending on which concat_mainDVs produced above)\n",
    "tasks = ['CUE','PREDICT','DF','NBACK','FLANKER','SHAPE','GNG','SS']\n",
    "meanDCsDf = pd.DataFrame(index=tasks, columns=tasks)\n",
    "meanDEsDf = pd.DataFrame(index=tasks, columns=tasks)\n",
    "#first generate the diagonal values, which are the SE values, then fill in means of DC and DE for each task cluster\n",
    "for row in tasks:\n",
    "    for col in tasks:\n",
    "        if row==col:\n",
    "            meanDCsDf.loc[row,col] = concat_mainDVs.loc['mean','SE_%s' %col]\n",
    "            meanDEsDf.loc[row,col] = concat_mainDVs.loc['mean','SE_%s' %col]\n",
    "        else:\n",
    "            meanDCsDf.loc[row,col] = concat_mainDVs.loc['mean','DC_%s_%s' %(row,col)]\n",
    "            meanDEsDf.loc[row,col] = concat_mainDVs.loc['mean','DE_%s_%s' %(row,col)]\n",
    "output = pd.concat([meanDCsDf,meanDEsDf],axis=0, keys=['DC','DE'])\n",
    "output.to_csv('/Users/tonyb/Desktop/meanDCsDEs_RTs_byConstructs.csv')\n",
    "\n",
    "#include the following 2 for loops if standardizing by SE values\n",
    "meanDCsDf_std = pd.DataFrame(index=tasks, columns=tasks)\n",
    "meanDEsDf_std = pd.DataFrame(index=tasks, columns=tasks)\n",
    "for row in tasks:\n",
    "    for col in tasks:\n",
    "        meanDCsDf_std.loc[row,col] = round(float(meanDCsDf.loc[row,col])/meanDCsDf.loc[row,row],3)\n",
    "        meanDEsDf_std.loc[row,col] = round(float(meanDEsDf.loc[row,col])/meanDEsDf.loc[row,row],3)\n",
    "output = pd.concat([meanDCsDf_std,meanDEsDf_std],axis=0, keys=['DC','DE'])\n",
    "output.to_csv('/Users/tonyb/Desktop/meanDCsDEs_RTs_byConstructs_stdBySEs.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap among all SE, DC, and DE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### correlations among all SE, DC, and DE data (from 33 explore subjs)\n",
    "### change ou\n",
    "concat_mainDVs_corr = round(concat_mainDVs.corr(),3)\n",
    "concat_mainDVs_corr.to_csv(main_DVs_analyses+'concat/concat_mainDVsRTs_corr_index=False.csv', index=False)\n",
    "# concat_mainDVs_corr.to_csv(main_DVs_analyses+'concat/concat_mainDVsAcc_corr_index=False.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### heatmap: 1's on diagonal replaced by means of corresponding raw data\n",
    "# annotData = pd.read_csv(main_DVs_analyses+'concat/concat_mainDVsRTs_corr_index=False.csv')\n",
    "annotData = pd.read_csv(main_DVs_analyses+'concat/concat_mainDVsAcc_corr_index=False.csv')\n",
    "annotData.index = annotData.columns\n",
    "for i in annotData.index:\n",
    "    for j in annotData.columns:\n",
    "        if i==j:\n",
    "            annotData.loc[i,j] = concat_mainDVs[j].mean()\n",
    "\n",
    "#data for drawing heatmap\n",
    "data = pd.read_csv(raw_dir+'main_DVs_analyses/concat/concat_mainDVsAcc_corr_index=False.csv') \n",
    "column_labels = data.columns\n",
    "row_labels = data.columns\n",
    "\n",
    "#set up background (fig) and main map (ax)\n",
    "fig, ax = plt.subplots(figsize=(170,170), facecolor='beige')\n",
    "ax.set_title('All DV variants Corr Matrix, DV=Accuracy',size=80)\n",
    "heatmap = ax.pcolor(data, cmap=cm.seismic, vmin=-1, vmax=1)\n",
    " \n",
    "# Put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(0.5,data.shape[1],1), minor=False)\n",
    "ax.set_yticks(np.arange(0.5,data.shape[0],1), minor=False)\n",
    "ax.tick_params(labelright=True, top=True, labeltop=True, bottom=False)\n",
    "\n",
    "for y in range(data.shape[0]):\n",
    "    for x in range(data.shape[1]):\n",
    "        text = ax.text(x + 0.5, y + 0.5, '%.3f' % annotData.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    " \n",
    "# Want a more natural, table-like display\n",
    "ax.axis('square')\n",
    "ax.invert_yaxis()\n",
    "# ax.xaxis.tick_top()\n",
    " \n",
    "ax.set_xticklabels(row_labels, minor=False, rotation=80, size=15)\n",
    "ax.set_yticklabels(column_labels, minor=False, size=15)\n",
    "\n",
    "# color bar\n",
    "cb = plt.colorbar(heatmap,shrink=0.5, aspect=30, fraction=.12, pad=.02)\n",
    "cb.ax.tick_params(labelsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-cluster by Task-cluster correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each cell has the average correlation of all SE, DC, and DE results of a task cluster with those of another task cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['CUE','PREDICT','DF','NBACK','FLANKER','SHAPE','GNG','SS']\n",
    "taskByTask_meanCorr_matrix = pd.DataFrame(index=tasks, columns=tasks)\n",
    "for i in tasks:\n",
    "    start_index = concat_mainDVs_corr.index.get_loc(i)\n",
    "    for j in tasks:\n",
    "        start_col = concat_mainDVs_corr.columns.get_loc(j)\n",
    "        #get the concat df\n",
    "        taskByTask_corrDf = concat_mainDVs_corr.iloc[start_index:start_index+15 , start_col:start_col+15] #because there're 15 variables for each task (1 SE, 7 DCs, 7 DEs)\n",
    "        #exclude 1's on the diagonal in correlation of the same 15 variants (i.e. of the same main_DV)\n",
    "        if i==j:\n",
    "            for a in range(taskByTask_corrDf.shape[0]):\n",
    "                for b in range(taskByTask_corrDf.shape[1]):\n",
    "                    if a==b:\n",
    "                        taskByTask_corrDf.iloc[a,b] = np.nan\n",
    "        #get average of all 15x15 corr numbers\n",
    "        taskByTask_corrDf_mean = taskByTask_corrDf.iloc[:,:].mean().mean()\n",
    "        taskByTask_meanCorr_matrix.loc[i,j] = round(taskByTask_corrDf_mean,3)\n",
    "taskByTask_meanCorr_matrix.to_csv(main_DVs_concat_dir + 'taskByTask_Accuracy_meanCorr_matrix.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot (heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(main_DVs_concat_dir+'taskByTask_Accuracy_meanCorr_matrix.csv')\n",
    "\n",
    "fig = plt.figure(figsize=(8,8),facecolor='beige')\n",
    "plt.title('Mean Correlation Matrix, DV=Accuracy',size=14,pad=15)\n",
    "plt.pcolor(df,cmap=cm.seismic,vmin=-1,vmax=1)\n",
    "plt.yticks(np.arange(0.5, len(df.columns), 1), df.columns, size=11)\n",
    "plt.xticks(np.arange(0.5, len(df.columns), 1), df.columns, rotation=70, size=11)\n",
    "plt.axis('square')\n",
    "for y in range(df.shape[0]):\n",
    "    for x in range(df.shape[1]):\n",
    "        plt.text(x + 0.5, y + 0.5, '%.3f' % df.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    "cb = plt.colorbar(shrink=0.8, aspect=12.5, fraction=.12, pad=.03)\n",
    "cb.ax.tick_params(labelsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate condition data without any subtraction (as with DC or DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate conditions (using RTs or Accuracy), no further subtractions as when calculating for DC or DE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "separate_conditions_dict = defaultdict(pd.DataFrame)\n",
    "concat_separate_conditions = pd.DataFrame()\n",
    "slowing_vs_effectChange = pd.DataFrame()\n",
    "choice = 1 #1: RTs, with SSRT; 2: RTs, with m(stopfail_rt), which is rt when a subj responded in a stop trial; 3: Accuracy\n",
    "\n",
    "### adding single tasks data first to its corresponding key in separate_conditions_dict\n",
    "# first line is regular, second is for use later in EZ-Diff model\n",
    "for qa_file in sorted(glob(explore_dir + 'QA/*single*')):\n",
    "# for qa_file in sorted(glob(explore_dir + 'QA_add0.5error_forACC=1/*single*')):\n",
    "    taskName = os.path.basename(qa_file).split('.')[0][:-3] #the last three characters are '_qa'\n",
    "    if 'no_go' in taskName: taskName = taskName.replace('no_go','nogo')\n",
    "    qaDf = pd.read_csv(qa_file, index_col=0)\n",
    "    if choice == 1:\n",
    "        if 'stop' in taskName:\n",
    "            qaDf = qaDf.filter(regex='m\\(rt\\)|ssrt')\n",
    "        else:\n",
    "            qaDf = qaDf.filter(regex='m\\(rt\\)')\n",
    "    elif choice == 2:\n",
    "        qaDf = qaDf.filter(regex='m\\(')\n",
    "    elif choice == 3:\n",
    "        qaDf = qaDf.filter(regex='acc')\n",
    "        \n",
    "    if 'stop' in taskName: #somehow only stop-signal qa files have all columns for the stop condition before go\n",
    "        qaDf_sortedCol = list(qaDf.columns)\n",
    "        qaDf_sortedCol.sort()\n",
    "        qaDf = qaDf[qaDf_sortedCol]\n",
    "    \n",
    "    for i in ['task:stay.*cue:switch','delay:3']:\n",
    "        qaDf = qaDf.loc[:,~qaDf.columns.str.contains(i, case=False)]\n",
    "    qaDf.columns = qaDf.columns.str.replace('\\.0','')\n",
    "    \n",
    "    #the immediately following line is the correct way of multiindexing columns\n",
    "#     qaDf.columns = pd.MultiIndex.from_product([[taskName],list(qaDf.columns)], names=['task_name','DV_condition'])\n",
    "    #the following 3 lines are to write with only one occurrence per level-0 column index (i.e., aesthetic)\n",
    "    multiIndex = list(np.repeat(' ', len(qaDf.columns)-1))\n",
    "    multiIndex.insert(0,taskName)\n",
    "    qaDf.columns = pd.MultiIndex.from_arrays([multiIndex, qaDf.columns], names=['task_name','DV_condition']) \n",
    "    \n",
    "    task = taskName.split('_single_')[0]\n",
    "    separate_conditions_dict[task] = pd.concat([separate_conditions_dict[task], qaDf], axis=1)\n",
    "\n",
    "# then adding dual tasks data to its corresponding key in separate_conditions_dict\n",
    "# for qa_file in sorted(glob(raw_dir + 'QA/*with*')):\n",
    "for qa_file in sorted(glob(raw_dir + 'QA_add0.5error_forACC=1/*with*')):\n",
    "    taskName = os.path.basename(qa_file).split('.')[0][:-3] #the last three characters are '_qa'\n",
    "    if 'no_go' in taskName: taskName = taskName.replace('no_go','nogo')\n",
    "    qaDf = pd.read_csv(qa_file, index_col=0)\n",
    "    qaDf = qaDf.filter(regex='m\\(')\n",
    "    if choice == 1:\n",
    "        if 'stop' in taskName:\n",
    "            qaDf = qaDf.filter(regex='m\\(rt\\)|ssrt')\n",
    "        else:\n",
    "            qaDf = qaDf.filter(regex='m\\(rt\\)')\n",
    "    elif choice == 2:\n",
    "        qaDf = qaDf.filter(regex='m\\(')\n",
    "    elif choice == 3:\n",
    "        qaDf = qaDf.filter(regex='acc')\n",
    "        \n",
    "    if 'stop' in taskName:\n",
    "        qaDf_sortedCol = list(qaDf.columns)\n",
    "        qaDf_sortedCol.sort()\n",
    "        qaDf = qaDf[qaDf_sortedCol]\n",
    "\n",
    "    for i in ['task:stay.*cue:switch','delay:3','m\\(nogofail_rt\\)_GNG:nogo_&_SS:stop']:\n",
    "        qaDf = qaDf.loc[:,~qaDf.columns.str.contains(i, case=False)]\n",
    "    qaDf.columns = qaDf.columns.str.replace('\\.0','')\n",
    "\n",
    "    #the immediately following line is the correct way of multiindexing columns\n",
    "#     qaDf.columns = pd.MultiIndex.from_product([[taskName],list(qaDf.columns)], names=['task_name','DV_condition'])\n",
    "    #the following 3 lines are to write with only one occurrence per level-0 column index (i.e., aesthetic)\n",
    "    multiIndex = list(np.repeat(' ', len(qaDf.columns)-1))\n",
    "    multiIndex.insert(0,taskName)\n",
    "    qaDf.columns = pd.MultiIndex.from_arrays([multiIndex, qaDf.columns], names=['task_name','DV_condition'])\n",
    "    \n",
    "    for i in taskName.split('_with_'):\n",
    "        separate_conditions_dict[i] = pd.concat([separate_conditions_dict[i], qaDf], axis=1)\n",
    "\n",
    "# writing each cluster of 8 tasks (1 single, 7 dual) data to a file; and concat them to a dataframe to write\n",
    "for key,val in separate_conditions_dict.items():\n",
    "    val.loc['std'] = round(val.iloc[:-1,:].std(),2)\n",
    "    #moving 'mean' and 'std' rows to top of dataframe\n",
    "    new_index = list(val.index[-2:]) + list(val.index[0:-2])\n",
    "    val = val.loc[new_index,:]\n",
    "    #The following (up to val = pd.concat) is to average across all dual tasks\n",
    "    val.columns = val.columns.droplevel(0)\n",
    "    for i in val.columns[:2]:\n",
    "        index = val.columns.get_loc(i)\n",
    "        new_col = '%s_dual' %i\n",
    "        if 'm(' in i:\n",
    "            if 'fail' in i:\n",
    "                val[new_col] = round(val.filter(regex='%s' %(val.columns[index][15:])).iloc[:,2:].mean(axis=1),2)\n",
    "            else:\n",
    "                val[new_col] = round(val.filter(regex='%s' %(val.columns[index][6:])).iloc[:,2:].mean(axis=1),2)\n",
    "    val = pd.concat([val.iloc[:,:2],val.iloc[:,-2:]],axis=1)\n",
    "    #calculate slowing (baseline dual minus baseline single) and effect-change (task-cost dual minus task-cost single)\n",
    "    val2 = pd.DataFrame()\n",
    "    val2['%s_slowing' %key] = val.iloc[:,2] - val.iloc[:,0]\n",
    "    val2['%s_effect' %key] = (val.iloc[:,3] - val.iloc[:,2]) - (val.iloc[:,1] - val.iloc[:,0])\n",
    "    #write\n",
    "    #val.to_csv(separate_conditions+'RTs/%s_RTs.csv' %key,index=True)\n",
    "    concat_separate_conditions = pd.concat([concat_separate_conditions, val], axis=1)\n",
    "    slowing_vs_effectChange = pd.concat([slowing_vs_effectChange,val2], axis=1)\n",
    "   \n",
    "if choice < 3:\n",
    "    # first line is regular, second is for use later in EZ-Diff model\n",
    "    concat_separate_conditions.to_csv(separate_conditions+'RTs/concat_separate_conditions_RTs.csv',index=True)\n",
    "    #concat_separate_conditions.to_csv(separate_conditions+'RTs/concat_separate_conditions_RTs_add0.5error_forACC=1.csv',index=True)\n",
    "    slowing_vs_effectChange.to_csv(separate_conditions+'condensed/slowing_vs_effectChange_RTs.csv', index=True)\n",
    "else:\n",
    "    concat_separate_conditions.to_csv(separate_conditions+'accuracy/concat_separate_conditions_Accuracy.csv',index=True)\n",
    "    #concat_separate_conditions.to_csv(separate_conditions+'accuracy/concat_separate_conditions_Accuracy_add0.5error_forACC=1.csv',index=True)\n",
    "    slowing_vs_effectChange.to_csv(separate_conditions+'condensed/slowing_vs_effectChange_Accuracy.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap for correlation b/w slowing and effect-change (RTs or Accuracy depending on the slowing_vs_effectChange df generated above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = slowing_vs_effectChange.iloc[2:,:] #first two rows are means and std's\n",
    "annotData = round(raw_data.corr(),4)\n",
    "annotData.index = annotData.columns\n",
    "for i in annotData.index:\n",
    "    for j in annotData.columns:\n",
    "        if i==j:\n",
    "            annotData.loc[i,j] = round(raw_data[j].mean(),4)\n",
    "\n",
    "#data for drawing heatmap\n",
    "data = round(raw_data.corr(),4) \n",
    "column_labels = data.columns\n",
    "row_labels = data.columns\n",
    "\n",
    "#set up background (fig) and main map (ax)\n",
    "fig, ax = plt.subplots(figsize=(20,20), facecolor='beige')\n",
    "ax.set_title('Slowing vs. Effect-change, DV=RTs',size=20)\n",
    "heatmap = ax.pcolor(data, cmap=cm.seismic, vmin=-1, vmax=1)\n",
    " \n",
    "# Put the major ticks at the middle of each cell\n",
    "ax.set_xticks(np.arange(0.5,data.shape[1],1), minor=False)\n",
    "ax.set_yticks(np.arange(0.5,data.shape[0],1), minor=False)\n",
    "ax.tick_params(labelright=True, top=True, labeltop=True, bottom=False)\n",
    "\n",
    "for y in range(data.shape[0]):\n",
    "    for x in range(data.shape[1]):\n",
    "        text = ax.text(x + 0.5, y + 0.5, '%.3f' % annotData.iloc[y, x],\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center',\n",
    "                 size=11)\n",
    " \n",
    "# Want a more natural, table-like display\n",
    "ax.axis('square')\n",
    "ax.invert_yaxis()\n",
    "# ax.xaxis.tick_top()\n",
    " \n",
    "ax.set_xticklabels(row_labels, minor=False, rotation=80, size=15)\n",
    "ax.set_yticklabels(column_labels, minor=False, size=15)\n",
    "\n",
    "# color bar\n",
    "cb = plt.colorbar(heatmap,shrink=0.5, aspect=30, fraction=.12, pad=.25)\n",
    "cb.ax.tick_params(labelsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for identical columns in separate-conditions data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#won't print if there are no idential columns\n",
    "for cluster, cluster_df in separate_conditions_dict.items():\n",
    "    print(cluster)\n",
    "    test = cluster_df\n",
    "    for i in range(len(test.columns)-1):\n",
    "        for j in range(i+1,len(test.columns)):\n",
    "            count = 0\n",
    "            for row in range(len(test)):\n",
    "                if test.iloc[row,i] == test.iloc[row,j] or np.isnan(test.iloc[row,i]) and np.isnan(test.iloc[row,j]): \n",
    "                    count +=1\n",
    "            if count == len(test): print('%s == %s' %(test.columns[i],test.columns[j]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
